{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook contains best performed kernels for Dog Breed Identifcation Kaggle Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the data from the following link\n",
    "\n",
    "Dog Images :https://www.kaggle.com/c/dog-breed-identification/data\n",
    "\n",
    "Download the dog dataset. Unzip the folder and place it in the repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anacondaa3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "from os import listdir, makedirs\n",
    "from os.path import join, exists, expanduser\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications import xception\n",
    "from keras.applications import inception_v3\n",
    "from keras.applications.vgg16 import preprocess_input, decode_predictions\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use top 16 classes\n",
    "\n",
    "Using all the images would take more than the 1 hour kernel limit. Let's focus on the most frequent 16 breeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10222 10222\n",
      "10357 10357\n"
     ]
    }
   ],
   "source": [
    "#INPUT_SIZE = 224\n",
    "NUM_CLASSES = 16\n",
    "SEED = 1987\n",
    "data_dir = 'C:/Users/Nishant/ADS/Project/Dog-Project-Kaggle Competition' # Path of your Project Folder  \n",
    "labels = pd.read_csv(join(data_dir, 'labels.csv'))\n",
    "sample_submission = pd.read_csv(join(data_dir, 'sample_submission.csv'))\n",
    "print(len(listdir(join(data_dir, 'train'))), len(labels))\n",
    "print(len(listdir(join(data_dir, 'test'))), len(sample_submission))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_breed_list = list(labels.groupby('breed').count().sort_values(by='id', ascending=False).head(NUM_CLASSES).index)\n",
    "labels = labels[labels['breed'].isin(selected_breed_list)]\n",
    "labels['target'] = 1\n",
    "labels['rank'] = labels.groupby('breed').rank()['id']\n",
    "labels_pivot = labels.pivot('id', 'breed', 'target').reset_index().fillna(0)\n",
    "np.random.seed(seed=SEED)\n",
    "rnd = np.random.random(len(labels))\n",
    "train_idx = rnd < 0.8\n",
    "valid_idx = rnd >= 0.8\n",
    "y_train = labels_pivot[selected_breed_list].values\n",
    "ytr = y_train[train_idx]\n",
    "yv = y_train[valid_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resizing the Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_img(img_id, train_or_test, size):\n",
    "    \"\"\"Read and resize image.\n",
    "    # Arguments\n",
    "        img_id: string\n",
    "        train_or_test: string 'train' or 'test'.\n",
    "        size: resize the original image.\n",
    "    # Returns\n",
    "        Image as numpy array.\n",
    "    \"\"\"\n",
    "    img = image.load_img(join(data_dir, train_or_test, '%s.jpg' % img_id), target_size=size)\n",
    "    img = image.img_to_array(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1777it [00:11, 151.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Images shape: (1777, 224, 224, 3) size: 267,488,256\n"
     ]
    }
   ],
   "source": [
    "INPUT_SIZE = 224\n",
    "POOLING = 'avg'\n",
    "x_train = np.zeros((len(labels), INPUT_SIZE, INPUT_SIZE, 3), dtype='float32')\n",
    "for i, img_id in tqdm(enumerate(labels['id'])):\n",
    "    img = read_img(img_id, 'train', (INPUT_SIZE, INPUT_SIZE))\n",
    "    x = preprocess_input(np.expand_dims(img.copy(), axis=0))\n",
    "    x_train[i] = x\n",
    "print('Train Images shape: {} size: {:,}'.format(x_train.shape, x_train.size))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract Resnet50 bottleneck features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((1409, 224, 224, 3), (368, 224, 224, 3), (1409, 16), (368, 16))\n",
      "1409/1409 [==============================] - ETA: 20:1 - ETA: 19:0 - ETA: 18:2 - ETA: 17:5 - ETA: 17:2 - ETA: 16:4 - ETA: 16:0 - ETA: 15:2 - ETA: 14:5 - ETA: 14:1 - ETA: 13:5 - ETA: 13:2 - ETA: 12:5 - ETA: 12:2 - ETA: 11:5 - ETA: 11:3 - ETA: 11:0 - ETA: 10:4 - ETA: 10:1 - ETA: 9:4 - ETA: 9: - ETA: 8: - ETA: 8: - ETA: 8: - ETA: 7: - ETA: 7: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 5: - ETA: 5: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 3: - ETA: 3: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 1: - ETA: 1: - ETA: 48s - ETA: 24 - ETA: 0 - 1059s 752ms/step\n",
      "368/368 [==============================] - ETA: 4: - ETA: 3: - ETA: 3: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 1: - ETA: 1: - ETA: 59s - ETA: 35 - ETA: 11 - 272s 740ms/step\n",
      "Resnet50 train bottleneck features shape: (1409, 2048) size: 2,885,632\n",
      "Resnet50 valid bottleneck features shape: (368, 2048) size: 753,664\n"
     ]
    }
   ],
   "source": [
    "Xtr = x_train[train_idx]\n",
    "Xv = x_train[valid_idx]\n",
    "print((Xtr.shape, Xv.shape, ytr.shape, yv.shape))\n",
    "resnet50_bottleneck = ResNet50(weights='imagenet', include_top=False, pooling=POOLING)\n",
    "train_resnet50_bf = resnet50_bottleneck.predict(Xtr, batch_size=32, verbose=1)\n",
    "valid_resnet50_bf = resnet50_bottleneck.predict(Xv, batch_size=32, verbose=1)\n",
    "print('Resnet50 train bottleneck features shape: {} size: {:,}'.format(train_resnet50_bf.shape, train_resnet50_bf.size))\n",
    "print('Resnet50 valid bottleneck features shape: {} size: {:,}'.format(valid_resnet50_bf.shape, valid_resnet50_bf.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LogReg on Resnet50 bottleneck features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(multi_class='multinomial', solver='lbfgs', random_state=SEED)\n",
    "logreg.fit(train_resnet50_bf, (ytr * range(NUM_CLASSES)).sum(axis=1))\n",
    "valid_probs = logreg.predict_proba(valid_resnet50_bf)\n",
    "valid_preds = logreg.predict(valid_resnet50_bf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Resnet50 LogLoss 0.19952801389568334\n",
      "Validation Resnet50 Accuracy 0.9402173913043478\n"
     ]
    }
   ],
   "source": [
    "print('Validation Resnet50 LogLoss {}'.format(log_loss(yv, valid_probs)))\n",
    "print('Validation Resnet50 Accuracy {}'.format(accuracy_score((yv * range(NUM_CLASSES)).sum(axis=1), valid_preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resnet50 Accuracy is 94%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract VGG16 bottleneck features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((1409, 224, 224, 3), (368, 224, 224, 3), (1409, 16), (368, 16))\n",
      "1409/1409 [==============================] - ETA: 10:5 - ETA: 10:4 - ETA: 10:2 - ETA: 10:1 - ETA: 9:5 - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 8: - ETA: 8: - ETA: 8: - ETA: 8: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 46s - ETA: 31 - ETA: 15 - ETA: 0 - 676s 480ms/step\n",
      "368/368 [==============================] - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 53s - ETA: 38 - ETA: 23 - ETA: 7 - 177s 481ms/step\n",
      "VGG train bottleneck features shape: (1409, 512) size: 721,408\n",
      "VGG valid bottleneck features shape: (368, 512) size: 188,416\n"
     ]
    }
   ],
   "source": [
    "Xtr = x_train[train_idx]\n",
    "Xv = x_train[valid_idx]\n",
    "print((Xtr.shape, Xv.shape, ytr.shape, yv.shape))\n",
    "vgg_bottleneck = VGG16(weights='imagenet', include_top=False, pooling=POOLING)\n",
    "train_vgg_bf = vgg_bottleneck.predict(Xtr, batch_size=32, verbose=1)\n",
    "valid_vgg_bf = vgg_bottleneck.predict(Xv, batch_size=32, verbose=1)\n",
    "print('VGG train bottleneck features shape: {} size: {:,}'.format(train_vgg_bf.shape, train_vgg_bf.size))\n",
    "print('VGG valid bottleneck features shape: {} size: {:,}'.format(valid_vgg_bf.shape, valid_vgg_bf.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LogReg on VGG16 bottleneck features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "logregVgg16 = LogisticRegression(multi_class='multinomial', solver='lbfgs', random_state=SEED)\n",
    "logregVgg16.fit(train_vgg_bf, (ytr * range(NUM_CLASSES)).sum(axis=1))\n",
    "valid_probsVgg16 = logregVgg16.predict_proba(valid_vgg_bf)\n",
    "valid_predsVgg16 = logregVgg16.predict(valid_vgg_bf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation VGG16 LogLoss 0.35206014208503983\n",
      "Validation VGG16 Accuracy 0.9184782608695652\n"
     ]
    }
   ],
   "source": [
    "print('Validation VGG16 LogLoss {}'.format(log_loss(yv, valid_probsVgg16)))\n",
    "print('Validation VGG16 Accuracy {}'.format(accuracy_score((yv * range(NUM_CLASSES)).sum(axis=1), valid_predsVgg16)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG16 Accuracy is 91.84%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract VGG16 bottleneck features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((1409, 224, 224, 3), (368, 224, 224, 3), (1409, 16), (368, 16))\n",
      "1409/1409 [==============================] - ETA: 13:3 - ETA: 13:1 - ETA: 12:5 - ETA: 12:3 - ETA: 12:1 - ETA: 12:0 - ETA: 11:4 - ETA: 11:2 - ETA: 11:0 - ETA: 10:4 - ETA: 10:2 - ETA: 10:0 - ETA: 9:4 - ETA: 9: - ETA: 9: - ETA: 8: - ETA: 8: - ETA: 8: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 57s - ETA: 38 - ETA: 19 - ETA: 0 - 832s 591ms/step\n",
      "368/368 [==============================] - ETA: 3: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 47s - ETA: 28 - ETA: 9 - 218s 591ms/step\n",
      "VGG19 train bottleneck features shape: (1409, 512) size: 721,408\n",
      "VGG19 valid bottleneck features shape: (368, 512) size: 188,416\n"
     ]
    }
   ],
   "source": [
    "Xtr = x_train[train_idx]\n",
    "Xv = x_train[valid_idx]\n",
    "print((Xtr.shape, Xv.shape, ytr.shape, yv.shape))\n",
    "vgg19_bottleneck = VGG19(weights='imagenet', include_top=False, pooling=POOLING)\n",
    "train_vgg19_bf = vgg19_bottleneck.predict(Xtr, batch_size=32, verbose=1)\n",
    "valid_vgg19_bf = vgg19_bottleneck.predict(Xv, batch_size=32, verbose=1)\n",
    "print('VGG19 train bottleneck features shape: {} size: {:,}'.format(train_vgg19_bf.shape, train_vgg19_bf.size))\n",
    "print('VGG19 valid bottleneck features shape: {} size: {:,}'.format(valid_vgg19_bf.shape, valid_vgg19_bf.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LogReg on VGG16 bottleneck features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "logregVgg19 = LogisticRegression(multi_class='multinomial', solver='lbfgs', random_state=SEED)\n",
    "logregVgg19.fit(train_vgg19_bf, (ytr * range(NUM_CLASSES)).sum(axis=1))\n",
    "valid_probsVgg19 = logregVgg19.predict_proba(valid_vgg19_bf)\n",
    "valid_predsVgg19 = logregVgg19.predict(valid_vgg19_bf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation VGG19 LogLoss 0.33411898533364837\n",
      "Validation VGG19 Accuracy 0.9239130434782609\n"
     ]
    }
   ],
   "source": [
    "print('Validation VGG19 LogLoss {}'.format(log_loss(yv, valid_probsVgg19)))\n",
    "print('Validation VGG19 Accuracy {}'.format(accuracy_score((yv * range(NUM_CLASSES)).sum(axis=1), valid_predsVgg19)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " VGG19 Accuracy is 92.39%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1777it [00:18, 95.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Images shape: (1777, 299, 299, 3) size: 476,596,731\n"
     ]
    }
   ],
   "source": [
    "INPUT_SIZE = 299\n",
    "POOLING = 'avg'\n",
    "x_trainX = np.zeros((len(labels), INPUT_SIZE, INPUT_SIZE, 3), dtype='float32')\n",
    "for i, img_id in tqdm(enumerate(labels['id'])):\n",
    "    img = read_img(img_id, 'train', (INPUT_SIZE, INPUT_SIZE))\n",
    "    x = xception.preprocess_input(np.expand_dims(img.copy(), axis=0))\n",
    "    x_trainX[i] = x\n",
    "print('Train Images shape: {} size: {:,}'.format(x_trainX.shape, x_trainX.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract Xception bottleneck features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((1409, 299, 299, 3), (368, 299, 299, 3), (1409, 16), (368, 16))\n",
      "1409/1409 [==============================] - ETA: 33:2 - ETA: 32:1 - ETA: 31:2 - ETA: 30:3 - ETA: 29:4 - ETA: 28:5 - ETA: 28:1 - ETA: 27:2 - ETA: 26:4 - ETA: 25:5 - ETA: 25:0 - ETA: 24:2 - ETA: 23:3 - ETA: 22:5 - ETA: 22:0 - ETA: 21:2 - ETA: 20:3 - ETA: 19:4 - ETA: 19:0 - ETA: 18:1 - ETA: 17:3 - ETA: 16:4 - ETA: 16:0 - ETA: 15:1 - ETA: 14:3 - ETA: 13:4 - ETA: 13:0 - ETA: 12:1 - ETA: 11:2 - ETA: 10:4 - ETA: 9:5 - ETA: 9: - ETA: 8: - ETA: 7: - ETA: 6: - ETA: 6: - ETA: 5: - ETA: 4: - ETA: 3: - ETA: 3: - ETA: 2: - ETA: 1: - ETA: 47s - ETA: 1 - 2015s 1s/step\n",
      "368/368 [==============================] - ETA: 7: - ETA: 7: - ETA: 6: - ETA: 5: - ETA: 4: - ETA: 4: - ETA: 3: - ETA: 2: - ETA: 1: - ETA: 1: - ETA: 22s - 526s 1s/step\n",
      "Xception train bottleneck features shape: (1409, 2048) size: 2,885,632\n",
      "Xception valid bottleneck features shape: (368, 2048) size: 753,664\n"
     ]
    }
   ],
   "source": [
    "Xtr = x_trainX[train_idx]\n",
    "Xv = x_trainX[valid_idx]\n",
    "print((Xtr.shape, Xv.shape, ytr.shape, yv.shape))\n",
    "xception_bottleneck = xception.Xception(weights='imagenet', include_top=False, pooling=POOLING)\n",
    "train_x_bf = xception_bottleneck.predict(Xtr, batch_size=32, verbose=1)\n",
    "valid_x_bf = xception_bottleneck.predict(Xv, batch_size=32, verbose=1)\n",
    "print('Xception train bottleneck features shape: {} size: {:,}'.format(train_x_bf.shape, train_x_bf.size))\n",
    "print('Xception valid bottleneck features shape: {} size: {:,}'.format(valid_x_bf.shape, valid_x_bf.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LogReg on Xception bottleneck features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(multi_class='multinomial', solver='lbfgs', random_state=SEED)\n",
    "logreg.fit(train_x_bf, (ytr * range(NUM_CLASSES)).sum(axis=1))\n",
    "valid_probsX = logreg.predict_proba(valid_x_bf)\n",
    "valid_predsX = logreg.predict(valid_x_bf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Xception LogLoss 0.06829603188561884\n",
      "Validation Xception Accuracy 0.9809782608695652\n"
     ]
    }
   ],
   "source": [
    "print('Validation Xception LogLoss {}'.format(log_loss(yv, valid_probsX)))\n",
    "print('Validation Xception Accuracy {}'.format(accuracy_score((yv * range(NUM_CLASSES)).sum(axis=1), valid_predsX)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xception Accuracy is 98%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import keras\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will read in the csv's so we can see some more information on the filenames and breeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('C:/Users/Nishant/ADS/Project/Dog-Project-Kaggle Competition/labels.csv')\n",
    "# path of labels.csv inside your project folder\n",
    "\n",
    "df_test = pd.read_csv('C:/Users/Nishant/ADS/Project/Dog-Project-Kaggle Competition/sample_submission.csv')\n",
    "# path of sample_submission.csv inside your project folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>breed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000bec180eb18c7604dcecc8fe0dba07</td>\n",
       "      <td>boston_bull</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001513dfcb2ffafc82cccf4d8bbaba97</td>\n",
       "      <td>dingo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001cdf01b096e06d78e9e5112d419397</td>\n",
       "      <td>pekinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00214f311d5d2247d5dfe4fe24b2303d</td>\n",
       "      <td>bluetick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0021f9ceb3235effd7fcde7f7538ed62</td>\n",
       "      <td>golden_retriever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>002211c81b498ef88e1b40b9abf84e1d</td>\n",
       "      <td>bedlington_terrier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>00290d3e1fdd27226ba27a8ce248ce85</td>\n",
       "      <td>bedlington_terrier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>002a283a315af96eaea0e28e7163b21b</td>\n",
       "      <td>borzoi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>003df8b8a8b05244b1d920bb6cf451f9</td>\n",
       "      <td>basenji</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0042188c895a2f14ef64a918ed9c7b64</td>\n",
       "      <td>scottish_deerhound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>004396df1acd0f1247b740ca2b14616e</td>\n",
       "      <td>shetland_sheepdog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0067dc3eab0b3c3ef0439477624d85d6</td>\n",
       "      <td>walker_hound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>00693b8bc2470375cc744a6391d397ec</td>\n",
       "      <td>maltese_dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>006cc3ddb9dc1bd827479569fcdc52dc</td>\n",
       "      <td>bluetick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0075dc49dab4024d12fafe67074d8a81</td>\n",
       "      <td>norfolk_terrier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>00792e341f3c6eb33663e415d0715370</td>\n",
       "      <td>african_hunting_dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>007b5a16db9d9ff9d7ad39982703e429</td>\n",
       "      <td>wire-haired_fox_terrier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>007b8a07882822475a4ce6581e70b1f8</td>\n",
       "      <td>redbone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>007ff9a78eba2aebb558afea3a51c469</td>\n",
       "      <td>lakeland_terrier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>008887054b18ba3c7601792b6a453cc3</td>\n",
       "      <td>boxer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  id                    breed\n",
       "0   000bec180eb18c7604dcecc8fe0dba07              boston_bull\n",
       "1   001513dfcb2ffafc82cccf4d8bbaba97                    dingo\n",
       "2   001cdf01b096e06d78e9e5112d419397                 pekinese\n",
       "3   00214f311d5d2247d5dfe4fe24b2303d                 bluetick\n",
       "4   0021f9ceb3235effd7fcde7f7538ed62         golden_retriever\n",
       "5   002211c81b498ef88e1b40b9abf84e1d       bedlington_terrier\n",
       "6   00290d3e1fdd27226ba27a8ce248ce85       bedlington_terrier\n",
       "7   002a283a315af96eaea0e28e7163b21b                   borzoi\n",
       "8   003df8b8a8b05244b1d920bb6cf451f9                  basenji\n",
       "9   0042188c895a2f14ef64a918ed9c7b64       scottish_deerhound\n",
       "10  004396df1acd0f1247b740ca2b14616e        shetland_sheepdog\n",
       "11  0067dc3eab0b3c3ef0439477624d85d6             walker_hound\n",
       "12  00693b8bc2470375cc744a6391d397ec              maltese_dog\n",
       "13  006cc3ddb9dc1bd827479569fcdc52dc                 bluetick\n",
       "14  0075dc49dab4024d12fafe67074d8a81          norfolk_terrier\n",
       "15  00792e341f3c6eb33663e415d0715370      african_hunting_dog\n",
       "16  007b5a16db9d9ff9d7ad39982703e429  wire-haired_fox_terrier\n",
       "17  007b8a07882822475a4ce6581e70b1f8                  redbone\n",
       "18  007ff9a78eba2aebb558afea3a51c469         lakeland_terrier\n",
       "19  008887054b18ba3c7601792b6a453cc3                    boxer"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the breed needs to be one-hot encoded for the final submission, so we will now do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_series = pd.Series(df_train['breed'])\n",
    "one_hot = pd.get_dummies(targets_series, sparse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_labels = np.asarray(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will read in all of the images for test and train, using a for loop through the values of the csv files. I have also set an im_size variable which sets the size for the image to be re-sized to, 90x90 px, you should play with this number to see how it affects accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_size = 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "x_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 10222/10222 [01:23<00:00, 122.46it/s]\n"
     ]
    }
   ],
   "source": [
    "i = 0 \n",
    "for f, breed in tqdm(df_train.values):\n",
    "    img = cv2.imread('C:/Users/Nishant/ADS/Project/Dog-Project-Kaggle Competition/train/{}.jpg'.format(f))\n",
    "    # path of train folder inside your project folder\n",
    "    \n",
    "    label = one_hot_labels[i]\n",
    "    x_train.append(cv2.resize(img, (im_size, im_size)))\n",
    "    y_train.append(label)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 10357/10357 [01:28<00:00, 117.39it/s]\n"
     ]
    }
   ],
   "source": [
    "for f in tqdm(df_test['id'].values):\n",
    "    img = cv2.imread('C:/Users/Nishant/ADS/Project/Dog-Project-Kaggle Competition/test/{}.jpg'.format(f))\n",
    "    # path of test folder inside your project folder\n",
    "    \n",
    "    x_test.append(cv2.resize(img, (im_size, im_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_raw = np.array(y_train, np.uint8)\n",
    "x_train_raw = np.array(x_train, np.float32) / 255.\n",
    "x_test  = np.array(x_test, np.float32) / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check the shape of the outputs to make sure everyting went as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10222, 90, 90, 3)\n",
      "(10222, 120)\n",
      "(10357, 90, 90, 3)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_raw.shape)\n",
    "print(y_train_raw.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see above that there are 120 different breeds. We can put this in a num_class variable below that can then be used when creating the CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_class = y_train_raw.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to create a validation set so that you can gauge the performance of your model on independent data, unseen to the model in training. We do this by splitting the current training set (x_train_raw) and the corresponding labels (y_train_raw) so that we set aside 30 % of the data at random and put these in validation sets (X_valid and Y_valid).\n",
    "\n",
    "This split needs to be improved so that it contains images from every class, with 120 separate classes some can not be represented and so the validation score is not informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, Y_train, Y_valid = train_test_split(x_train_raw, y_train_raw, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build the CNN architecture. Here we are using a pre-trained model VGG19 which has already been trained to identify many different dog breeds (as well as a lot of other objects from the imagenet dataset see here for more information: http://image-net.org/about-overview). Unfortunately it doesn't seem possible to download the weights from within this kernel so make sure you set the weights argument to 'imagenet' and not None, as it currently is below.\n",
    "\n",
    "We then remove the final layer and instead replace it with a single dense layer with the number of nodes corresponding to the number of breed classes we have (120)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 90, 90, 3)         0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 90, 90, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 90, 90, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 45, 45, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 45, 45, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 45, 45, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 22, 22, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 22, 22, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 22, 22, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 22, 22, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 22, 22, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 11, 11, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 11, 11, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 11, 11, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 11, 11, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 11, 11, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 5, 5, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 5, 5, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 5, 5, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 5, 5, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, 5, 5, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 120)               245880    \n",
      "=================================================================\n",
      "Total params: 20,270,264\n",
      "Trainable params: 245,880\n",
      "Non-trainable params: 20,024,384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create the base pre-trained model\n",
    "# Can't download weights in the kernel\n",
    "base_model = VGG19(weights='imagenet',include_top=False, input_shape=(im_size, im_size, 3))\n",
    "\n",
    "# Add a new top layer\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "predictions = Dense(num_class, activation='softmax')(x)\n",
    "\n",
    "# This is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# First: train only the top layers (which were randomly initialized)\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "callbacks_list = [keras.callbacks.EarlyStopping(monitor='val_acc', patience=3, verbose=1)]\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7155 samples, validate on 3067 samples\n",
      "Epoch 1/2\n",
      "6464/7155 [==========================>...] - ETA: 18:42 - loss: 5.0450 - acc: 0.0000e+ - ETA: 14:35 - loss: 5.0728 - acc: 0.0000e+ - ETA: 13:02 - loss: 5.0894 - acc: 0.0208   - ETA: 12:15 - loss: 5.0539 - acc: 0.01 - ETA: 11:45 - loss: 5.1523 - acc: 0.01 - ETA: 11:25 - loss: 5.1513 - acc: 0.01 - ETA: 11:11 - loss: 5.2006 - acc: 0.01 - ETA: 10:58 - loss: 5.2166 - acc: 0.01 - ETA: 10:49 - loss: 5.2438 - acc: 0.01 - ETA: 10:40 - loss: 5.2296 - acc: 0.01 - ETA: 10:32 - loss: 5.2431 - acc: 0.01 - ETA: 10:25 - loss: 5.2379 - acc: 0.01 - ETA: 10:18 - loss: 5.2363 - acc: 0.01 - ETA: 10:12 - loss: 5.2751 - acc: 0.01 - ETA: 10:07 - loss: 5.2683 - acc: 0.01 - ETA: 10:02 - loss: 5.2463 - acc: 0.01 - ETA: 9:57 - loss: 5.2209 - acc: 0.0129 - ETA: 9:55 - loss: 5.2116 - acc: 0.013 - ETA: 9:52 - loss: 5.1880 - acc: 0.014 - ETA: 9:50 - loss: 5.1843 - acc: 0.017 - ETA: 9:48 - loss: 5.1755 - acc: 0.016 - ETA: 9:45 - loss: 5.1576 - acc: 0.017 - ETA: 9:42 - loss: 5.1501 - acc: 0.016 - ETA: 9:39 - loss: 5.1465 - acc: 0.015 - ETA: 9:37 - loss: 5.1439 - acc: 0.016 - ETA: 9:34 - loss: 5.1450 - acc: 0.015 - ETA: 9:32 - loss: 5.1411 - acc: 0.016 - ETA: 9:29 - loss: 5.1378 - acc: 0.015 - ETA: 9:26 - loss: 5.1291 - acc: 0.016 - ETA: 9:23 - loss: 5.1301 - acc: 0.015 - ETA: 9:21 - loss: 5.1170 - acc: 0.018 - ETA: 9:18 - loss: 5.1062 - acc: 0.017 - ETA: 9:15 - loss: 5.1011 - acc: 0.018 - ETA: 9:13 - loss: 5.1039 - acc: 0.017 - ETA: 9:10 - loss: 5.0980 - acc: 0.019 - ETA: 9:08 - loss: 5.0907 - acc: 0.019 - ETA: 9:06 - loss: 5.0797 - acc: 0.019 - ETA: 9:04 - loss: 5.0768 - acc: 0.019 - ETA: 9:03 - loss: 5.0737 - acc: 0.019 - ETA: 9:00 - loss: 5.0715 - acc: 0.018 - ETA: 8:58 - loss: 5.0653 - acc: 0.018 - ETA: 8:55 - loss: 5.0593 - acc: 0.017 - ETA: 8:54 - loss: 5.0561 - acc: 0.017 - ETA: 8:53 - loss: 5.0461 - acc: 0.017 - ETA: 8:51 - loss: 5.0417 - acc: 0.016 - ETA: 8:50 - loss: 5.0284 - acc: 0.016 - ETA: 8:48 - loss: 5.0239 - acc: 0.016 - ETA: 8:47 - loss: 5.0186 - acc: 0.018 - ETA: 8:45 - loss: 5.0131 - acc: 0.018 - ETA: 8:43 - loss: 5.0081 - acc: 0.018 - ETA: 8:42 - loss: 5.0024 - acc: 0.019 - ETA: 8:40 - loss: 4.9963 - acc: 0.019 - ETA: 8:40 - loss: 4.9936 - acc: 0.020 - ETA: 8:39 - loss: 4.9907 - acc: 0.020 - ETA: 8:38 - loss: 4.9863 - acc: 0.020 - ETA: 8:36 - loss: 4.9855 - acc: 0.020 - ETA: 8:35 - loss: 4.9795 - acc: 0.020 - ETA: 8:33 - loss: 4.9743 - acc: 0.019 - ETA: 8:30 - loss: 4.9677 - acc: 0.019 - ETA: 8:27 - loss: 4.9581 - acc: 0.020 - ETA: 8:24 - loss: 4.9535 - acc: 0.021 - ETA: 8:22 - loss: 4.9511 - acc: 0.020 - ETA: 8:20 - loss: 4.9479 - acc: 0.020 - ETA: 8:18 - loss: 4.9451 - acc: 0.020 - ETA: 8:17 - loss: 4.9424 - acc: 0.020 - ETA: 8:15 - loss: 4.9384 - acc: 0.021 - ETA: 8:13 - loss: 4.9340 - acc: 0.022 - ETA: 8:11 - loss: 4.9312 - acc: 0.023 - ETA: 8:09 - loss: 4.9247 - acc: 0.023 - ETA: 8:07 - loss: 4.9201 - acc: 0.024 - ETA: 8:05 - loss: 4.9175 - acc: 0.024 - ETA: 8:03 - loss: 4.9113 - acc: 0.024 - ETA: 8:01 - loss: 4.9077 - acc: 0.024 - ETA: 7:57 - loss: 4.9050 - acc: 0.024 - ETA: 7:54 - loss: 4.8989 - acc: 0.023 - ETA: 7:50 - loss: 4.8966 - acc: 0.023 - ETA: 7:46 - loss: 4.8935 - acc: 0.023 - ETA: 7:43 - loss: 4.8916 - acc: 0.024 - ETA: 7:39 - loss: 4.8871 - acc: 0.024 - ETA: 7:36 - loss: 4.8818 - acc: 0.025 - ETA: 7:34 - loss: 4.8810 - acc: 0.026 - ETA: 7:33 - loss: 4.8759 - acc: 0.027 - ETA: 7:32 - loss: 4.8731 - acc: 0.027 - ETA: 7:30 - loss: 4.8705 - acc: 0.026 - ETA: 7:28 - loss: 4.8681 - acc: 0.027 - ETA: 7:27 - loss: 4.8685 - acc: 0.026 - ETA: 7:24 - loss: 4.8682 - acc: 0.026 - ETA: 7:21 - loss: 4.8645 - acc: 0.027 - ETA: 7:17 - loss: 4.8604 - acc: 0.027 - ETA: 7:14 - loss: 4.8596 - acc: 0.027 - ETA: 7:11 - loss: 4.8556 - acc: 0.028 - ETA: 7:08 - loss: 4.8512 - acc: 0.028 - ETA: 7:05 - loss: 4.8470 - acc: 0.028 - ETA: 7:01 - loss: 4.8433 - acc: 0.028 - ETA: 6:58 - loss: 4.8394 - acc: 0.029 - ETA: 6:55 - loss: 4.8388 - acc: 0.030 - ETA: 6:51 - loss: 4.8369 - acc: 0.030 - ETA: 6:48 - loss: 4.8329 - acc: 0.030 - ETA: 6:45 - loss: 4.8305 - acc: 0.030 - ETA: 6:42 - loss: 4.8278 - acc: 0.030 - ETA: 6:39 - loss: 4.8238 - acc: 0.030 - ETA: 6:35 - loss: 4.8208 - acc: 0.030 - ETA: 6:32 - loss: 4.8208 - acc: 0.030 - ETA: 6:29 - loss: 4.8190 - acc: 0.030 - ETA: 6:25 - loss: 4.8178 - acc: 0.030 - ETA: 6:22 - loss: 4.8166 - acc: 0.029 - ETA: 6:18 - loss: 4.8145 - acc: 0.029 - ETA: 6:15 - loss: 4.8131 - acc: 0.029 - ETA: 6:12 - loss: 4.8102 - acc: 0.029 - ETA: 6:10 - loss: 4.8065 - acc: 0.029 - ETA: 6:08 - loss: 4.8035 - acc: 0.029 - ETA: 6:06 - loss: 4.7987 - acc: 0.031 - ETA: 6:04 - loss: 4.7944 - acc: 0.031 - ETA: 6:05 - loss: 4.7915 - acc: 0.032 - ETA: 6:02 - loss: 4.7904 - acc: 0.032 - ETA: 5:59 - loss: 4.7880 - acc: 0.031 - ETA: 5:56 - loss: 4.7860 - acc: 0.031 - ETA: 5:53 - loss: 4.7815 - acc: 0.032 - ETA: 5:50 - loss: 4.7786 - acc: 0.032 - ETA: 5:47 - loss: 4.7743 - acc: 0.032 - ETA: 5:44 - loss: 4.7712 - acc: 0.032 - ETA: 5:41 - loss: 4.7691 - acc: 0.033 - ETA: 5:38 - loss: 4.7666 - acc: 0.033 - ETA: 5:34 - loss: 4.7636 - acc: 0.032 - ETA: 5:31 - loss: 4.7604 - acc: 0.033 - ETA: 5:28 - loss: 4.7590 - acc: 0.033 - ETA: 5:25 - loss: 4.7549 - acc: 0.034 - ETA: 5:21 - loss: 4.7527 - acc: 0.033 - ETA: 5:18 - loss: 4.7512 - acc: 0.034 - ETA: 5:14 - loss: 4.7492 - acc: 0.034 - ETA: 5:11 - loss: 4.7473 - acc: 0.034 - ETA: 5:08 - loss: 4.7435 - acc: 0.035 - ETA: 5:05 - loss: 4.7423 - acc: 0.035 - ETA: 5:02 - loss: 4.7393 - acc: 0.035 - ETA: 4:59 - loss: 4.7349 - acc: 0.036 - ETA: 4:55 - loss: 4.7363 - acc: 0.036 - ETA: 4:52 - loss: 4.7334 - acc: 0.036 - ETA: 4:49 - loss: 4.7327 - acc: 0.036 - ETA: 4:46 - loss: 4.7314 - acc: 0.036 - ETA: 4:44 - loss: 4.7291 - acc: 0.037 - ETA: 4:41 - loss: 4.7260 - acc: 0.037 - ETA: 4:38 - loss: 4.7233 - acc: 0.037 - ETA: 4:35 - loss: 4.7215 - acc: 0.037 - ETA: 4:32 - loss: 4.7199 - acc: 0.037 - ETA: 4:28 - loss: 4.7168 - acc: 0.038 - ETA: 4:25 - loss: 4.7156 - acc: 0.038 - ETA: 4:21 - loss: 4.7144 - acc: 0.038 - ETA: 4:18 - loss: 4.7134 - acc: 0.038 - ETA: 4:14 - loss: 4.7125 - acc: 0.038 - ETA: 4:11 - loss: 4.7110 - acc: 0.038 - ETA: 4:08 - loss: 4.7085 - acc: 0.038 - ETA: 4:04 - loss: 4.7076 - acc: 0.038 - ETA: 4:01 - loss: 4.7076 - acc: 0.038 - ETA: 3:57 - loss: 4.7058 - acc: 0.038 - ETA: 3:54 - loss: 4.7050 - acc: 0.038 - ETA: 3:51 - loss: 4.7032 - acc: 0.038 - ETA: 3:47 - loss: 4.7000 - acc: 0.039 - ETA: 3:44 - loss: 4.6999 - acc: 0.039 - ETA: 3:41 - loss: 4.6967 - acc: 0.040 - ETA: 3:37 - loss: 4.6942 - acc: 0.040 - ETA: 3:34 - loss: 4.6919 - acc: 0.040 - ETA: 3:30 - loss: 4.6898 - acc: 0.040 - ETA: 3:27 - loss: 4.6885 - acc: 0.041 - ETA: 3:23 - loss: 4.6871 - acc: 0.041 - ETA: 3:20 - loss: 4.6849 - acc: 0.041 - ETA: 3:16 - loss: 4.6830 - acc: 0.041 - ETA: 3:13 - loss: 4.6824 - acc: 0.041 - ETA: 3:09 - loss: 4.6812 - acc: 0.041 - ETA: 3:06 - loss: 4.6787 - acc: 0.041 - ETA: 3:02 - loss: 4.6778 - acc: 0.041 - ETA: 2:59 - loss: 4.6735 - acc: 0.042 - ETA: 2:56 - loss: 4.6732 - acc: 0.042 - ETA: 2:52 - loss: 4.6713 - acc: 0.042 - ETA: 2:49 - loss: 4.6713 - acc: 0.042 - ETA: 2:46 - loss: 4.6702 - acc: 0.042 - ETA: 2:42 - loss: 4.6707 - acc: 0.042 - ETA: 2:39 - loss: 4.6689 - acc: 0.043 - ETA: 2:35 - loss: 4.6670 - acc: 0.043 - ETA: 2:32 - loss: 4.6663 - acc: 0.043 - ETA: 2:29 - loss: 4.6649 - acc: 0.043 - ETA: 2:25 - loss: 4.6644 - acc: 0.043 - ETA: 2:22 - loss: 4.6598 - acc: 0.044 - ETA: 2:18 - loss: 4.6566 - acc: 0.045 - ETA: 2:15 - loss: 4.6540 - acc: 0.045 - ETA: 2:12 - loss: 4.6517 - acc: 0.045 - ETA: 2:08 - loss: 4.6508 - acc: 0.046 - ETA: 2:05 - loss: 4.6492 - acc: 0.046 - ETA: 2:02 - loss: 4.6485 - acc: 0.046 - ETA: 1:58 - loss: 4.6481 - acc: 0.046 - ETA: 1:55 - loss: 4.6460 - acc: 0.046 - ETA: 1:51 - loss: 4.6457 - acc: 0.046 - ETA: 1:48 - loss: 4.6426 - acc: 0.046 - ETA: 1:45 - loss: 4.6409 - acc: 0.046 - ETA: 1:41 - loss: 4.6405 - acc: 0.046 - ETA: 1:38 - loss: 4.6399 - acc: 0.046 - ETA: 1:34 - loss: 4.6399 - acc: 0.046 - ETA: 1:31 - loss: 4.6394 - acc: 0.046 - ETA: 1:28 - loss: 4.6382 - acc: 0.046 - ETA: 1:24 - loss: 4.6377 - acc: 0.046 - ETA: 1:21 - loss: 4.6334 - acc: 0.047 - ETA: 1:17 - loss: 4.6328 - acc: 0.047 - ETA: 1:14 - loss: 4.6309 - acc: 0.0467155/7155 [==============================] - ETA: 1:10 - loss: 4.6294 - acc: 0.046 - ETA: 1:07 - loss: 4.6264 - acc: 0.047 - ETA: 1:04 - loss: 4.6245 - acc: 0.048 - ETA: 1:00 - loss: 4.6238 - acc: 0.048 - ETA: 57s - loss: 4.6231 - acc: 0.048 - ETA: 53s - loss: 4.6237 - acc: 0.04 - ETA: 50s - loss: 4.6213 - acc: 0.04 - ETA: 47s - loss: 4.6211 - acc: 0.04 - ETA: 43s - loss: 4.6188 - acc: 0.04 - ETA: 40s - loss: 4.6161 - acc: 0.04 - ETA: 36s - loss: 4.6137 - acc: 0.04 - ETA: 33s - loss: 4.6133 - acc: 0.04 - ETA: 30s - loss: 4.6123 - acc: 0.04 - ETA: 26s - loss: 4.6097 - acc: 0.04 - ETA: 23s - loss: 4.6090 - acc: 0.04 - ETA: 19s - loss: 4.6090 - acc: 0.05 - ETA: 16s - loss: 4.6074 - acc: 0.05 - ETA: 12s - loss: 4.6064 - acc: 0.05 - ETA: 9s - loss: 4.6066 - acc: 0.0501 - ETA: 5s - loss: 4.6058 - acc: 0.050 - ETA: 2s - loss: 4.6046 - acc: 0.049 - 1196s 167ms/step - loss: 4.6032 - acc: 0.0499 - val_loss: 4.2596 - val_acc: 0.0871\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6528/7155 [==========================>...] - ETA: 13:17 - loss: 3.4765 - acc: 0.25 - ETA: 12:06 - loss: 3.6223 - acc: 0.18 - ETA: 11:47 - loss: 3.6237 - acc: 0.19 - ETA: 11:41 - loss: 3.6819 - acc: 0.17 - ETA: 11:37 - loss: 3.6344 - acc: 0.20 - ETA: 11:31 - loss: 3.5978 - acc: 0.20 - ETA: 11:28 - loss: 3.6420 - acc: 0.20 - ETA: 11:21 - loss: 3.6556 - acc: 0.19 - ETA: 11:19 - loss: 3.6579 - acc: 0.20 - ETA: 11:17 - loss: 3.6886 - acc: 0.20 - ETA: 11:15 - loss: 3.6190 - acc: 0.22 - ETA: 11:10 - loss: 3.6329 - acc: 0.21 - ETA: 11:05 - loss: 3.6430 - acc: 0.20 - ETA: 11:02 - loss: 3.6653 - acc: 0.20 - ETA: 10:58 - loss: 3.6831 - acc: 0.20 - ETA: 10:53 - loss: 3.6739 - acc: 0.20 - ETA: 10:50 - loss: 3.6653 - acc: 0.20 - ETA: 10:45 - loss: 3.6727 - acc: 0.20 - ETA: 10:42 - loss: 3.6779 - acc: 0.19 - ETA: 10:39 - loss: 3.6897 - acc: 0.19 - ETA: 10:35 - loss: 3.6936 - acc: 0.19 - ETA: 10:32 - loss: 3.7042 - acc: 0.19 - ETA: 10:29 - loss: 3.7047 - acc: 0.19 - ETA: 10:26 - loss: 3.7019 - acc: 0.19 - ETA: 10:23 - loss: 3.6923 - acc: 0.20 - ETA: 10:19 - loss: 3.6822 - acc: 0.20 - ETA: 10:17 - loss: 3.6864 - acc: 0.20 - ETA: 10:12 - loss: 3.6833 - acc: 0.20 - ETA: 10:09 - loss: 3.6791 - acc: 0.20 - ETA: 10:06 - loss: 3.6734 - acc: 0.20 - ETA: 10:03 - loss: 3.6774 - acc: 0.20 - ETA: 10:00 - loss: 3.6643 - acc: 0.20 - ETA: 9:57 - loss: 3.6670 - acc: 0.2017 - ETA: 9:54 - loss: 3.6748 - acc: 0.198 - ETA: 9:51 - loss: 3.6716 - acc: 0.201 - ETA: 9:48 - loss: 3.6593 - acc: 0.204 - ETA: 9:46 - loss: 3.6583 - acc: 0.203 - ETA: 9:43 - loss: 3.6505 - acc: 0.203 - ETA: 9:41 - loss: 3.6446 - acc: 0.204 - ETA: 9:38 - loss: 3.6467 - acc: 0.202 - ETA: 9:34 - loss: 3.6465 - acc: 0.203 - ETA: 9:32 - loss: 3.6500 - acc: 0.202 - ETA: 9:29 - loss: 3.6493 - acc: 0.202 - ETA: 9:25 - loss: 3.6511 - acc: 0.200 - ETA: 9:22 - loss: 3.6526 - acc: 0.197 - ETA: 9:19 - loss: 3.6452 - acc: 0.199 - ETA: 9:15 - loss: 3.6578 - acc: 0.198 - ETA: 9:13 - loss: 3.6592 - acc: 0.198 - ETA: 9:10 - loss: 3.6604 - acc: 0.197 - ETA: 9:08 - loss: 3.6615 - acc: 0.196 - ETA: 9:05 - loss: 3.6562 - acc: 0.199 - ETA: 9:02 - loss: 3.6553 - acc: 0.198 - ETA: 8:59 - loss: 3.6564 - acc: 0.199 - ETA: 8:55 - loss: 3.6546 - acc: 0.199 - ETA: 8:52 - loss: 3.6531 - acc: 0.199 - ETA: 8:49 - loss: 3.6503 - acc: 0.200 - ETA: 8:45 - loss: 3.6572 - acc: 0.200 - ETA: 8:42 - loss: 3.6614 - acc: 0.199 - ETA: 8:39 - loss: 3.6663 - acc: 0.199 - ETA: 8:36 - loss: 3.6696 - acc: 0.197 - ETA: 8:32 - loss: 3.6697 - acc: 0.197 - ETA: 8:29 - loss: 3.6719 - acc: 0.196 - ETA: 8:26 - loss: 3.6718 - acc: 0.196 - ETA: 8:23 - loss: 3.6711 - acc: 0.195 - ETA: 8:20 - loss: 3.6753 - acc: 0.194 - ETA: 8:16 - loss: 3.6788 - acc: 0.193 - ETA: 8:13 - loss: 3.6807 - acc: 0.192 - ETA: 8:10 - loss: 3.6791 - acc: 0.191 - ETA: 8:07 - loss: 3.6804 - acc: 0.192 - ETA: 8:04 - loss: 3.6779 - acc: 0.192 - ETA: 8:01 - loss: 3.6758 - acc: 0.192 - ETA: 7:58 - loss: 3.6770 - acc: 0.191 - ETA: 7:56 - loss: 3.6765 - acc: 0.191 - ETA: 7:52 - loss: 3.6738 - acc: 0.192 - ETA: 7:49 - loss: 3.6738 - acc: 0.192 - ETA: 7:46 - loss: 3.6726 - acc: 0.192 - ETA: 7:43 - loss: 3.6712 - acc: 0.192 - ETA: 7:40 - loss: 3.6709 - acc: 0.192 - ETA: 7:37 - loss: 3.6702 - acc: 0.191 - ETA: 7:34 - loss: 3.6749 - acc: 0.192 - ETA: 7:31 - loss: 3.6794 - acc: 0.191 - ETA: 7:28 - loss: 3.6745 - acc: 0.192 - ETA: 7:24 - loss: 3.6754 - acc: 0.192 - ETA: 7:21 - loss: 3.6758 - acc: 0.192 - ETA: 7:17 - loss: 3.6775 - acc: 0.192 - ETA: 7:14 - loss: 3.6768 - acc: 0.192 - ETA: 7:11 - loss: 3.6761 - acc: 0.192 - ETA: 7:07 - loss: 3.6718 - acc: 0.192 - ETA: 7:04 - loss: 3.6717 - acc: 0.192 - ETA: 7:01 - loss: 3.6706 - acc: 0.191 - ETA: 6:57 - loss: 3.6710 - acc: 0.191 - ETA: 6:54 - loss: 3.6698 - acc: 0.190 - ETA: 6:51 - loss: 3.6701 - acc: 0.190 - ETA: 6:48 - loss: 3.6712 - acc: 0.189 - ETA: 6:44 - loss: 3.6735 - acc: 0.189 - ETA: 6:41 - loss: 3.6727 - acc: 0.188 - ETA: 6:38 - loss: 3.6720 - acc: 0.188 - ETA: 6:35 - loss: 3.6674 - acc: 0.189 - ETA: 6:31 - loss: 3.6700 - acc: 0.189 - ETA: 6:28 - loss: 3.6699 - acc: 0.189 - ETA: 6:25 - loss: 3.6694 - acc: 0.189 - ETA: 6:21 - loss: 3.6670 - acc: 0.190 - ETA: 6:18 - loss: 3.6660 - acc: 0.189 - ETA: 6:15 - loss: 3.6634 - acc: 0.191 - ETA: 6:12 - loss: 3.6649 - acc: 0.190 - ETA: 6:09 - loss: 3.6612 - acc: 0.191 - ETA: 6:05 - loss: 3.6580 - acc: 0.191 - ETA: 6:02 - loss: 3.6558 - acc: 0.192 - ETA: 5:59 - loss: 3.6549 - acc: 0.192 - ETA: 5:56 - loss: 3.6545 - acc: 0.192 - ETA: 5:52 - loss: 3.6565 - acc: 0.192 - ETA: 5:49 - loss: 3.6587 - acc: 0.191 - ETA: 5:46 - loss: 3.6581 - acc: 0.192 - ETA: 5:43 - loss: 3.6568 - acc: 0.192 - ETA: 5:40 - loss: 3.6571 - acc: 0.192 - ETA: 5:37 - loss: 3.6567 - acc: 0.192 - ETA: 5:33 - loss: 3.6580 - acc: 0.191 - ETA: 5:30 - loss: 3.6573 - acc: 0.191 - ETA: 5:27 - loss: 3.6559 - acc: 0.192 - ETA: 5:24 - loss: 3.6568 - acc: 0.192 - ETA: 5:21 - loss: 3.6571 - acc: 0.193 - ETA: 5:18 - loss: 3.6574 - acc: 0.192 - ETA: 5:14 - loss: 3.6555 - acc: 0.193 - ETA: 5:11 - loss: 3.6558 - acc: 0.193 - ETA: 5:08 - loss: 3.6570 - acc: 0.192 - ETA: 5:05 - loss: 3.6582 - acc: 0.191 - ETA: 5:02 - loss: 3.6573 - acc: 0.191 - ETA: 4:58 - loss: 3.6595 - acc: 0.190 - ETA: 4:55 - loss: 3.6586 - acc: 0.191 - ETA: 4:52 - loss: 3.6587 - acc: 0.190 - ETA: 4:49 - loss: 3.6588 - acc: 0.191 - ETA: 4:46 - loss: 3.6580 - acc: 0.191 - ETA: 4:43 - loss: 3.6566 - acc: 0.192 - ETA: 4:39 - loss: 3.6569 - acc: 0.192 - ETA: 4:36 - loss: 3.6559 - acc: 0.192 - ETA: 4:33 - loss: 3.6571 - acc: 0.191 - ETA: 4:30 - loss: 3.6560 - acc: 0.192 - ETA: 4:27 - loss: 3.6541 - acc: 0.192 - ETA: 4:24 - loss: 3.6538 - acc: 0.192 - ETA: 4:20 - loss: 3.6530 - acc: 0.192 - ETA: 4:17 - loss: 3.6524 - acc: 0.192 - ETA: 4:14 - loss: 3.6544 - acc: 0.191 - ETA: 4:11 - loss: 3.6551 - acc: 0.191 - ETA: 4:08 - loss: 3.6543 - acc: 0.191 - ETA: 4:04 - loss: 3.6551 - acc: 0.191 - ETA: 4:01 - loss: 3.6563 - acc: 0.190 - ETA: 3:58 - loss: 3.6538 - acc: 0.191 - ETA: 3:55 - loss: 3.6542 - acc: 0.191 - ETA: 3:52 - loss: 3.6535 - acc: 0.191 - ETA: 3:49 - loss: 3.6534 - acc: 0.191 - ETA: 3:45 - loss: 3.6536 - acc: 0.192 - ETA: 3:42 - loss: 3.6511 - acc: 0.193 - ETA: 3:39 - loss: 3.6524 - acc: 0.192 - ETA: 3:36 - loss: 3.6516 - acc: 0.192 - ETA: 3:33 - loss: 3.6504 - acc: 0.192 - ETA: 3:30 - loss: 3.6495 - acc: 0.192 - ETA: 3:27 - loss: 3.6474 - acc: 0.193 - ETA: 3:23 - loss: 3.6482 - acc: 0.193 - ETA: 3:20 - loss: 3.6491 - acc: 0.192 - ETA: 3:17 - loss: 3.6472 - acc: 0.193 - ETA: 3:14 - loss: 3.6461 - acc: 0.193 - ETA: 3:11 - loss: 3.6461 - acc: 0.193 - ETA: 3:08 - loss: 3.6453 - acc: 0.193 - ETA: 3:04 - loss: 3.6445 - acc: 0.193 - ETA: 3:01 - loss: 3.6450 - acc: 0.193 - ETA: 2:58 - loss: 3.6461 - acc: 0.192 - ETA: 2:55 - loss: 3.6474 - acc: 0.192 - ETA: 2:52 - loss: 3.6481 - acc: 0.192 - ETA: 2:49 - loss: 3.6477 - acc: 0.192 - ETA: 2:46 - loss: 3.6457 - acc: 0.192 - ETA: 2:42 - loss: 3.6453 - acc: 0.193 - ETA: 2:39 - loss: 3.6452 - acc: 0.193 - ETA: 2:36 - loss: 3.6445 - acc: 0.193 - ETA: 2:33 - loss: 3.6442 - acc: 0.193 - ETA: 2:30 - loss: 3.6442 - acc: 0.193 - ETA: 2:27 - loss: 3.6446 - acc: 0.193 - ETA: 2:24 - loss: 3.6435 - acc: 0.193 - ETA: 2:21 - loss: 3.6421 - acc: 0.194 - ETA: 2:17 - loss: 3.6397 - acc: 0.194 - ETA: 2:14 - loss: 3.6394 - acc: 0.194 - ETA: 2:11 - loss: 3.6403 - acc: 0.194 - ETA: 2:08 - loss: 3.6385 - acc: 0.194 - ETA: 2:05 - loss: 3.6396 - acc: 0.194 - ETA: 2:02 - loss: 3.6419 - acc: 0.194 - ETA: 1:59 - loss: 3.6399 - acc: 0.195 - ETA: 1:56 - loss: 3.6391 - acc: 0.195 - ETA: 1:52 - loss: 3.6402 - acc: 0.195 - ETA: 1:49 - loss: 3.6386 - acc: 0.195 - ETA: 1:46 - loss: 3.6374 - acc: 0.195 - ETA: 1:43 - loss: 3.6380 - acc: 0.195 - ETA: 1:40 - loss: 3.6360 - acc: 0.194 - ETA: 1:37 - loss: 3.6354 - acc: 0.194 - ETA: 1:34 - loss: 3.6330 - acc: 0.195 - ETA: 1:31 - loss: 3.6326 - acc: 0.195 - ETA: 1:28 - loss: 3.6336 - acc: 0.195 - ETA: 1:25 - loss: 3.6324 - acc: 0.195 - ETA: 1:22 - loss: 3.6322 - acc: 0.195 - ETA: 1:18 - loss: 3.6307 - acc: 0.196 - ETA: 1:15 - loss: 3.6294 - acc: 0.196 - ETA: 1:12 - loss: 3.6301 - acc: 0.196 - ETA: 1:09 - loss: 3.6285 - acc: 0.196 - ETA: 1:06 - loss: 3.6277 - acc: 0.196 - ETA: 1:03 - loss: 3.6263 - acc: 0.196 - ETA: 1:00 - loss: 3.6255 - acc: 0.1968"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7155/7155 [==============================] - ETA: 57s - loss: 3.6255 - acc: 0.197 - ETA: 54s - loss: 3.6254 - acc: 0.19 - ETA: 51s - loss: 3.6258 - acc: 0.19 - ETA: 48s - loss: 3.6258 - acc: 0.19 - ETA: 44s - loss: 3.6243 - acc: 0.19 - ETA: 41s - loss: 3.6236 - acc: 0.19 - ETA: 38s - loss: 3.6230 - acc: 0.19 - ETA: 35s - loss: 3.6211 - acc: 0.19 - ETA: 32s - loss: 3.6222 - acc: 0.19 - ETA: 29s - loss: 3.6213 - acc: 0.19 - ETA: 26s - loss: 3.6213 - acc: 0.19 - ETA: 23s - loss: 3.6207 - acc: 0.19 - ETA: 20s - loss: 3.6208 - acc: 0.19 - ETA: 17s - loss: 3.6201 - acc: 0.19 - ETA: 14s - loss: 3.6191 - acc: 0.19 - ETA: 11s - loss: 3.6155 - acc: 0.19 - ETA: 7s - loss: 3.6155 - acc: 0.1982 - ETA: 4s - loss: 3.6158 - acc: 0.197 - ETA: 1s - loss: 3.6170 - acc: 0.197 - 969s 135ms/step - loss: 3.6169 - acc: 0.1978 - val_loss: 4.1062 - val_acc: 0.1125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2199ec76f28>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, epochs=2, validation_data=(X_valid, Y_valid), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, accuracy is low here because we are not taking advantage of the pre-trained weights as they cannot be downloaded in the kernel. This means we are training the wights from scratch.\n",
    "Next we will make our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10357/10357 [==============================] - ETA: 1:16:0 - ETA: 45:14  - ETA: 34:5 - ETA: 29:4 - ETA: 26:3 - ETA: 24:3 - ETA: 23:0 - ETA: 21:5 - ETA: 21:0 - ETA: 20:2 - ETA: 19:4 - ETA: 19:1 - ETA: 18:5 - ETA: 18:3 - ETA: 18:1 - ETA: 18:0 - ETA: 17:5 - ETA: 17:3 - ETA: 17:2 - ETA: 17:1 - ETA: 17:0 - ETA: 16:5 - ETA: 16:4 - ETA: 16:4 - ETA: 16:3 - ETA: 16:2 - ETA: 16:1 - ETA: 16:1 - ETA: 16:0 - ETA: 15:5 - ETA: 15:5 - ETA: 15:4 - ETA: 15:4 - ETA: 15:3 - ETA: 15:3 - ETA: 15:2 - ETA: 15:2 - ETA: 15:1 - ETA: 15:1 - ETA: 15:0 - ETA: 15:0 - ETA: 14:5 - ETA: 14:5 - ETA: 14:4 - ETA: 14:4 - ETA: 14:4 - ETA: 14:3 - ETA: 14:3 - ETA: 14:2 - ETA: 14:2 - ETA: 14:1 - ETA: 14:1 - ETA: 14:1 - ETA: 14:0 - ETA: 14:0 - ETA: 13:5 - ETA: 13:5 - ETA: 13:4 - ETA: 13:4 - ETA: 13:4 - ETA: 13:3 - ETA: 13:3 - ETA: 13:3 - ETA: 13:2 - ETA: 13:2 - ETA: 13:1 - ETA: 13:1 - ETA: 13:1 - ETA: 13:0 - ETA: 13:0 - ETA: 13:0 - ETA: 12:5 - ETA: 12:5 - ETA: 12:4 - ETA: 12:4 - ETA: 12:4 - ETA: 12:3 - ETA: 12:3 - ETA: 12:3 - ETA: 12:2 - ETA: 12:2 - ETA: 12:2 - ETA: 12:1 - ETA: 12:1 - ETA: 12:1 - ETA: 12:0 - ETA: 12:0 - ETA: 12:0 - ETA: 11:5 - ETA: 11:5 - ETA: 11:5 - ETA: 11:4 - ETA: 11:4 - ETA: 11:4 - ETA: 11:3 - ETA: 11:3 - ETA: 11:3 - ETA: 11:2 - ETA: 11:2 - ETA: 11:2 - ETA: 11:1 - ETA: 11:1 - ETA: 11:1 - ETA: 11:0 - ETA: 11:0 - ETA: 11:0 - ETA: 10:5 - ETA: 10:5 - ETA: 10:5 - ETA: 10:4 - ETA: 10:4 - ETA: 10:4 - ETA: 10:3 - ETA: 10:3 - ETA: 10:3 - ETA: 10:3 - ETA: 10:2 - ETA: 10:2 - ETA: 10:2 - ETA: 10:1 - ETA: 10:1 - ETA: 10:1 - ETA: 10:0 - ETA: 10:0 - ETA: 10:0 - ETA: 9:5 - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 8: - ETA: 8: - ETA: 8: - ETA: 8: - ETA: 8: - ETA: 8: - ETA: 8: - ETA: 8: - ETA: 8: - ETA: 8: - ETA: 8: - ETA: 8: - ETA: 8: - ETA: 8: - ETA: 8: - ETA: 8: - ETA: 8: - ETA: 8: - ETA: 8: - ETA: 8: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 58s - ETA: 55 - ETA: 52 - ETA: 49 - ETA: 46 - ETA: 43 - ETA: 40 - ETA: 37 - ETA: 34 - ETA: 31 - ETA: 28 - ETA: 25 - ETA: 22 - ETA: 19 - ETA: 16 - ETA: 13 - ETA: 10 - ETA: 7 - ETA:  - ETA:  - 966s 93ms/step\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(x_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>affenpinscher</th>\n",
       "      <th>afghan_hound</th>\n",
       "      <th>african_hunting_dog</th>\n",
       "      <th>airedale</th>\n",
       "      <th>american_staffordshire_terrier</th>\n",
       "      <th>appenzeller</th>\n",
       "      <th>australian_terrier</th>\n",
       "      <th>basenji</th>\n",
       "      <th>basset</th>\n",
       "      <th>...</th>\n",
       "      <th>toy_poodle</th>\n",
       "      <th>toy_terrier</th>\n",
       "      <th>vizsla</th>\n",
       "      <th>walker_hound</th>\n",
       "      <th>weimaraner</th>\n",
       "      <th>welsh_springer_spaniel</th>\n",
       "      <th>west_highland_white_terrier</th>\n",
       "      <th>whippet</th>\n",
       "      <th>wire-haired_fox_terrier</th>\n",
       "      <th>yorkshire_terrier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000621fb3cbb32d8935728e48679680e</td>\n",
       "      <td>0.000432</td>\n",
       "      <td>0.014376</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.000199</td>\n",
       "      <td>0.003398</td>\n",
       "      <td>0.006372</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.000456</td>\n",
       "      <td>0.014755</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002897</td>\n",
       "      <td>0.000293</td>\n",
       "      <td>2.223149e-03</td>\n",
       "      <td>0.004878</td>\n",
       "      <td>1.300617e-02</td>\n",
       "      <td>0.128114</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.005594</td>\n",
       "      <td>0.002639</td>\n",
       "      <td>0.000252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00102ee9d8eb90812350685311fe5890</td>\n",
       "      <td>0.010025</td>\n",
       "      <td>0.011054</td>\n",
       "      <td>0.001197</td>\n",
       "      <td>0.000980</td>\n",
       "      <td>0.000880</td>\n",
       "      <td>0.001622</td>\n",
       "      <td>0.000850</td>\n",
       "      <td>0.000740</td>\n",
       "      <td>0.001548</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015734</td>\n",
       "      <td>0.013045</td>\n",
       "      <td>1.127672e-03</td>\n",
       "      <td>0.001126</td>\n",
       "      <td>1.402815e-03</td>\n",
       "      <td>0.004256</td>\n",
       "      <td>0.022141</td>\n",
       "      <td>0.001812</td>\n",
       "      <td>0.034709</td>\n",
       "      <td>0.000671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0012a730dfa437f5f3613fb75efcd4ce</td>\n",
       "      <td>0.005741</td>\n",
       "      <td>0.012840</td>\n",
       "      <td>0.008748</td>\n",
       "      <td>0.026019</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.015596</td>\n",
       "      <td>0.001174</td>\n",
       "      <td>0.011600</td>\n",
       "      <td>0.009769</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002168</td>\n",
       "      <td>0.003151</td>\n",
       "      <td>6.573818e-03</td>\n",
       "      <td>0.006059</td>\n",
       "      <td>7.069316e-03</td>\n",
       "      <td>0.007737</td>\n",
       "      <td>0.010484</td>\n",
       "      <td>0.035557</td>\n",
       "      <td>0.013938</td>\n",
       "      <td>0.001120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>001510bc8570bbeee98c8d80c8a95ec1</td>\n",
       "      <td>0.014196</td>\n",
       "      <td>0.050318</td>\n",
       "      <td>0.004079</td>\n",
       "      <td>0.017362</td>\n",
       "      <td>0.001310</td>\n",
       "      <td>0.027249</td>\n",
       "      <td>0.004624</td>\n",
       "      <td>0.015408</td>\n",
       "      <td>0.001825</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002208</td>\n",
       "      <td>0.005994</td>\n",
       "      <td>1.593215e-03</td>\n",
       "      <td>0.000987</td>\n",
       "      <td>1.795378e-03</td>\n",
       "      <td>0.004550</td>\n",
       "      <td>0.000740</td>\n",
       "      <td>0.028329</td>\n",
       "      <td>0.002532</td>\n",
       "      <td>0.001250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>001a5f3114548acdefa3d4da05474c2e</td>\n",
       "      <td>0.026999</td>\n",
       "      <td>0.019718</td>\n",
       "      <td>0.019079</td>\n",
       "      <td>0.026609</td>\n",
       "      <td>0.003240</td>\n",
       "      <td>0.003740</td>\n",
       "      <td>0.007579</td>\n",
       "      <td>0.000614</td>\n",
       "      <td>0.002492</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001686</td>\n",
       "      <td>0.000252</td>\n",
       "      <td>1.461654e-03</td>\n",
       "      <td>0.000753</td>\n",
       "      <td>5.949779e-03</td>\n",
       "      <td>0.000804</td>\n",
       "      <td>0.002326</td>\n",
       "      <td>0.011626</td>\n",
       "      <td>0.013219</td>\n",
       "      <td>0.001830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>00225dcd3e4d2410dd53239f95c0352f</td>\n",
       "      <td>0.000469</td>\n",
       "      <td>0.060080</td>\n",
       "      <td>0.001679</td>\n",
       "      <td>0.001781</td>\n",
       "      <td>0.000583</td>\n",
       "      <td>0.000275</td>\n",
       "      <td>0.000751</td>\n",
       "      <td>0.000906</td>\n",
       "      <td>0.009265</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003914</td>\n",
       "      <td>0.000879</td>\n",
       "      <td>1.451997e-03</td>\n",
       "      <td>0.024050</td>\n",
       "      <td>3.880257e-03</td>\n",
       "      <td>0.011223</td>\n",
       "      <td>0.004553</td>\n",
       "      <td>0.026240</td>\n",
       "      <td>0.016837</td>\n",
       "      <td>0.000668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>002c2a3117c2193b4d26400ce431eebd</td>\n",
       "      <td>0.013544</td>\n",
       "      <td>0.015975</td>\n",
       "      <td>0.016380</td>\n",
       "      <td>0.021049</td>\n",
       "      <td>0.000963</td>\n",
       "      <td>0.011972</td>\n",
       "      <td>0.011309</td>\n",
       "      <td>0.005278</td>\n",
       "      <td>0.001484</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006300</td>\n",
       "      <td>0.022877</td>\n",
       "      <td>6.666494e-03</td>\n",
       "      <td>0.003686</td>\n",
       "      <td>2.988879e-03</td>\n",
       "      <td>0.017697</td>\n",
       "      <td>0.017521</td>\n",
       "      <td>0.004451</td>\n",
       "      <td>0.018060</td>\n",
       "      <td>0.010736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>002c58d413a521ae8d1a5daeb35fc803</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.002018</td>\n",
       "      <td>0.000213</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>4.703510e-07</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>5.842896e-08</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>0.251009</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.209298</td>\n",
       "      <td>0.000351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>002f80396f1e3db687c5932d7978b196</td>\n",
       "      <td>0.014547</td>\n",
       "      <td>0.012317</td>\n",
       "      <td>0.050596</td>\n",
       "      <td>0.026392</td>\n",
       "      <td>0.002017</td>\n",
       "      <td>0.002739</td>\n",
       "      <td>0.003309</td>\n",
       "      <td>0.002362</td>\n",
       "      <td>0.002399</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001970</td>\n",
       "      <td>0.000582</td>\n",
       "      <td>3.288176e-03</td>\n",
       "      <td>0.000757</td>\n",
       "      <td>1.799287e-02</td>\n",
       "      <td>0.004378</td>\n",
       "      <td>0.003574</td>\n",
       "      <td>0.039597</td>\n",
       "      <td>0.006125</td>\n",
       "      <td>0.001191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0036c6bcec6031be9e62a257b1c3c442</td>\n",
       "      <td>0.006683</td>\n",
       "      <td>0.005048</td>\n",
       "      <td>0.006135</td>\n",
       "      <td>0.004311</td>\n",
       "      <td>0.001558</td>\n",
       "      <td>0.015824</td>\n",
       "      <td>0.000939</td>\n",
       "      <td>0.000266</td>\n",
       "      <td>0.006063</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.000505</td>\n",
       "      <td>2.212686e-03</td>\n",
       "      <td>0.004147</td>\n",
       "      <td>3.593173e-02</td>\n",
       "      <td>0.004372</td>\n",
       "      <td>0.000542</td>\n",
       "      <td>0.002859</td>\n",
       "      <td>0.003648</td>\n",
       "      <td>0.000926</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 121 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id  affenpinscher  afghan_hound  \\\n",
       "0  000621fb3cbb32d8935728e48679680e       0.000432      0.014376   \n",
       "1  00102ee9d8eb90812350685311fe5890       0.010025      0.011054   \n",
       "2  0012a730dfa437f5f3613fb75efcd4ce       0.005741      0.012840   \n",
       "3  001510bc8570bbeee98c8d80c8a95ec1       0.014196      0.050318   \n",
       "4  001a5f3114548acdefa3d4da05474c2e       0.026999      0.019718   \n",
       "5  00225dcd3e4d2410dd53239f95c0352f       0.000469      0.060080   \n",
       "6  002c2a3117c2193b4d26400ce431eebd       0.013544      0.015975   \n",
       "7  002c58d413a521ae8d1a5daeb35fc803       0.000024      0.000060   \n",
       "8  002f80396f1e3db687c5932d7978b196       0.014547      0.012317   \n",
       "9  0036c6bcec6031be9e62a257b1c3c442       0.006683      0.005048   \n",
       "\n",
       "   african_hunting_dog  airedale  american_staffordshire_terrier  appenzeller  \\\n",
       "0             0.000085  0.000199                        0.003398     0.006372   \n",
       "1             0.001197  0.000980                        0.000880     0.001622   \n",
       "2             0.008748  0.026019                        0.001400     0.015596   \n",
       "3             0.004079  0.017362                        0.001310     0.027249   \n",
       "4             0.019079  0.026609                        0.003240     0.003740   \n",
       "5             0.001679  0.001781                        0.000583     0.000275   \n",
       "6             0.016380  0.021049                        0.000963     0.011972   \n",
       "7             0.000002  0.000026                        0.000192     0.000070   \n",
       "8             0.050596  0.026392                        0.002017     0.002739   \n",
       "9             0.006135  0.004311                        0.001558     0.015824   \n",
       "\n",
       "   australian_terrier   basenji    basset        ...          toy_poodle  \\\n",
       "0            0.000152  0.000456  0.014755        ...            0.002897   \n",
       "1            0.000850  0.000740  0.001548        ...            0.015734   \n",
       "2            0.001174  0.011600  0.009769        ...            0.002168   \n",
       "3            0.004624  0.015408  0.001825        ...            0.002208   \n",
       "4            0.007579  0.000614  0.002492        ...            0.001686   \n",
       "5            0.000751  0.000906  0.009265        ...            0.003914   \n",
       "6            0.011309  0.005278  0.001484        ...            0.006300   \n",
       "7            0.002018  0.000213  0.000003        ...            0.000081   \n",
       "8            0.003309  0.002362  0.002399        ...            0.001970   \n",
       "9            0.000939  0.000266  0.006063        ...            0.001022   \n",
       "\n",
       "   toy_terrier        vizsla  walker_hound    weimaraner  \\\n",
       "0     0.000293  2.223149e-03      0.004878  1.300617e-02   \n",
       "1     0.013045  1.127672e-03      0.001126  1.402815e-03   \n",
       "2     0.003151  6.573818e-03      0.006059  7.069316e-03   \n",
       "3     0.005994  1.593215e-03      0.000987  1.795378e-03   \n",
       "4     0.000252  1.461654e-03      0.000753  5.949779e-03   \n",
       "5     0.000879  1.451997e-03      0.024050  3.880257e-03   \n",
       "6     0.022877  6.666494e-03      0.003686  2.988879e-03   \n",
       "7     0.000066  4.703510e-07      0.000002  5.842896e-08   \n",
       "8     0.000582  3.288176e-03      0.000757  1.799287e-02   \n",
       "9     0.000505  2.212686e-03      0.004147  3.593173e-02   \n",
       "\n",
       "   welsh_springer_spaniel  west_highland_white_terrier   whippet  \\\n",
       "0                0.128114                     0.000111  0.005594   \n",
       "1                0.004256                     0.022141  0.001812   \n",
       "2                0.007737                     0.010484  0.035557   \n",
       "3                0.004550                     0.000740  0.028329   \n",
       "4                0.000804                     0.002326  0.011626   \n",
       "5                0.011223                     0.004553  0.026240   \n",
       "6                0.017697                     0.017521  0.004451   \n",
       "7                0.000167                     0.251009  0.000001   \n",
       "8                0.004378                     0.003574  0.039597   \n",
       "9                0.004372                     0.000542  0.002859   \n",
       "\n",
       "   wire-haired_fox_terrier  yorkshire_terrier  \n",
       "0                 0.002639           0.000252  \n",
       "1                 0.034709           0.000671  \n",
       "2                 0.013938           0.001120  \n",
       "3                 0.002532           0.001250  \n",
       "4                 0.013219           0.001830  \n",
       "5                 0.016837           0.000668  \n",
       "6                 0.018060           0.010736  \n",
       "7                 0.209298           0.000351  \n",
       "8                 0.006125           0.001191  \n",
       "9                 0.003648           0.000926  \n",
       "\n",
       "[10 rows x 121 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub = pd.DataFrame(preds)\n",
    "# Set column names to those generated by the one-hot encoding earlier\n",
    "col_names = one_hot.columns.values\n",
    "sub.columns = col_names\n",
    "# Insert the column id from the sample_submission at the start of the data frame\n",
    "sub.insert(0, 'id', df_test['id'])\n",
    "sub.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have used the same code above again but this time with VGG16\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 90, 90, 3)         0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 90, 90, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 90, 90, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 45, 45, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 45, 45, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 45, 45, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 22, 22, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 22, 22, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 22, 22, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 22, 22, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 11, 11, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 11, 11, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 11, 11, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 11, 11, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 5, 5, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 5, 5, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 5, 5, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 5, 5, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 120)               245880    \n",
      "=================================================================\n",
      "Total params: 14,960,568\n",
      "Trainable params: 245,880\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create the base pre-trained model\n",
    "# Can't download weights in the kernel\n",
    "base_model = VGG16(weights='imagenet',include_top=False, input_shape=(im_size, im_size, 3))\n",
    "\n",
    "# Add a new top layer\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "predictions = Dense(num_class, activation='softmax')(x)\n",
    "\n",
    "# This is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# First: train only the top layers (which were randomly initialized)\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "callbacks_list = [keras.callbacks.EarlyStopping(monitor='val_acc', patience=3, verbose=1)]\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7155 samples, validate on 3067 samples\n",
      "Epoch 1/2\n",
      "6496/7155 [==========================>...] - ETA: 23:34 - loss: 5.1576 - acc: 0.03 - ETA: 16:28 - loss: 5.2267 - acc: 0.01 - ETA: 14:13 - loss: 5.2736 - acc: 0.01 - ETA: 13:00 - loss: 5.2298 - acc: 0.00 - ETA: 12:00 - loss: 5.1313 - acc: 0.00 - ETA: 11:18 - loss: 5.1025 - acc: 0.00 - ETA: 10:48 - loss: 5.0818 - acc: 0.00 - ETA: 10:25 - loss: 5.1411 - acc: 0.00 - ETA: 10:06 - loss: 5.1309 - acc: 0.00 - ETA: 9:51 - loss: 5.1701 - acc: 0.0063 - ETA: 9:40 - loss: 5.2221 - acc: 0.005 - ETA: 9:31 - loss: 5.2105 - acc: 0.005 - ETA: 9:24 - loss: 5.2025 - acc: 0.009 - ETA: 9:17 - loss: 5.2016 - acc: 0.008 - ETA: 9:10 - loss: 5.1925 - acc: 0.010 - ETA: 9:05 - loss: 5.1868 - acc: 0.013 - ETA: 8:59 - loss: 5.2046 - acc: 0.014 - ETA: 8:54 - loss: 5.1999 - acc: 0.013 - ETA: 8:49 - loss: 5.1894 - acc: 0.013 - ETA: 8:44 - loss: 5.1859 - acc: 0.012 - ETA: 8:40 - loss: 5.1662 - acc: 0.011 - ETA: 8:36 - loss: 5.1622 - acc: 0.011 - ETA: 8:32 - loss: 5.1371 - acc: 0.014 - ETA: 8:28 - loss: 5.1253 - acc: 0.018 - ETA: 8:24 - loss: 5.1132 - acc: 0.017 - ETA: 8:21 - loss: 5.1046 - acc: 0.016 - ETA: 8:17 - loss: 5.0919 - acc: 0.017 - ETA: 8:13 - loss: 5.0932 - acc: 0.016 - ETA: 8:10 - loss: 5.0932 - acc: 0.018 - ETA: 8:07 - loss: 5.0858 - acc: 0.019 - ETA: 8:04 - loss: 5.0878 - acc: 0.019 - ETA: 8:00 - loss: 5.0744 - acc: 0.018 - ETA: 7:57 - loss: 5.0626 - acc: 0.018 - ETA: 7:54 - loss: 5.0549 - acc: 0.018 - ETA: 7:51 - loss: 5.0475 - acc: 0.018 - ETA: 7:48 - loss: 5.0474 - acc: 0.018 - ETA: 7:45 - loss: 5.0441 - acc: 0.018 - ETA: 7:42 - loss: 5.0447 - acc: 0.018 - ETA: 7:39 - loss: 5.0322 - acc: 0.020 - ETA: 7:36 - loss: 5.0240 - acc: 0.020 - ETA: 7:33 - loss: 5.0244 - acc: 0.020 - ETA: 7:30 - loss: 5.0187 - acc: 0.020 - ETA: 7:27 - loss: 5.0151 - acc: 0.020 - ETA: 7:25 - loss: 5.0082 - acc: 0.022 - ETA: 7:22 - loss: 5.0027 - acc: 0.022 - ETA: 7:19 - loss: 4.9984 - acc: 0.023 - ETA: 7:16 - loss: 4.9969 - acc: 0.022 - ETA: 7:13 - loss: 4.9889 - acc: 0.024 - ETA: 7:11 - loss: 4.9919 - acc: 0.023 - ETA: 7:08 - loss: 4.9897 - acc: 0.023 - ETA: 7:06 - loss: 4.9867 - acc: 0.023 - ETA: 7:03 - loss: 4.9814 - acc: 0.024 - ETA: 7:00 - loss: 4.9790 - acc: 0.023 - ETA: 6:58 - loss: 4.9767 - acc: 0.023 - ETA: 6:55 - loss: 4.9730 - acc: 0.023 - ETA: 6:52 - loss: 4.9700 - acc: 0.023 - ETA: 6:50 - loss: 4.9662 - acc: 0.023 - ETA: 6:47 - loss: 4.9595 - acc: 0.022 - ETA: 6:44 - loss: 4.9541 - acc: 0.022 - ETA: 6:42 - loss: 4.9484 - acc: 0.021 - ETA: 6:39 - loss: 4.9410 - acc: 0.021 - ETA: 6:37 - loss: 4.9340 - acc: 0.022 - ETA: 6:34 - loss: 4.9312 - acc: 0.022 - ETA: 6:31 - loss: 4.9283 - acc: 0.023 - ETA: 6:29 - loss: 4.9199 - acc: 0.024 - ETA: 6:26 - loss: 4.9090 - acc: 0.025 - ETA: 6:24 - loss: 4.9034 - acc: 0.026 - ETA: 6:21 - loss: 4.8990 - acc: 0.026 - ETA: 6:18 - loss: 4.8964 - acc: 0.027 - ETA: 6:16 - loss: 4.8959 - acc: 0.027 - ETA: 6:13 - loss: 4.8926 - acc: 0.027 - ETA: 6:11 - loss: 4.8882 - acc: 0.027 - ETA: 6:09 - loss: 4.8872 - acc: 0.027 - ETA: 6:06 - loss: 4.8839 - acc: 0.027 - ETA: 6:04 - loss: 4.8795 - acc: 0.027 - ETA: 6:01 - loss: 4.8745 - acc: 0.028 - ETA: 5:59 - loss: 4.8723 - acc: 0.028 - ETA: 5:56 - loss: 4.8693 - acc: 0.028 - ETA: 5:54 - loss: 4.8625 - acc: 0.028 - ETA: 5:51 - loss: 4.8580 - acc: 0.029 - ETA: 5:49 - loss: 4.8556 - acc: 0.029 - ETA: 5:46 - loss: 4.8527 - acc: 0.030 - ETA: 5:44 - loss: 4.8484 - acc: 0.030 - ETA: 5:41 - loss: 4.8455 - acc: 0.030 - ETA: 5:39 - loss: 4.8429 - acc: 0.032 - ETA: 5:36 - loss: 4.8385 - acc: 0.032 - ETA: 5:34 - loss: 4.8325 - acc: 0.032 - ETA: 5:31 - loss: 4.8282 - acc: 0.032 - ETA: 5:29 - loss: 4.8207 - acc: 0.034 - ETA: 5:26 - loss: 4.8164 - acc: 0.035 - ETA: 5:24 - loss: 4.8139 - acc: 0.035 - ETA: 5:21 - loss: 4.8090 - acc: 0.035 - ETA: 5:19 - loss: 4.8016 - acc: 0.036 - ETA: 5:16 - loss: 4.7994 - acc: 0.036 - ETA: 5:14 - loss: 4.7965 - acc: 0.036 - ETA: 5:11 - loss: 4.7953 - acc: 0.036 - ETA: 5:09 - loss: 4.7927 - acc: 0.036 - ETA: 5:06 - loss: 4.7913 - acc: 0.036 - ETA: 5:04 - loss: 4.7878 - acc: 0.037 - ETA: 5:01 - loss: 4.7827 - acc: 0.037 - ETA: 4:59 - loss: 4.7769 - acc: 0.038 - ETA: 4:57 - loss: 4.7742 - acc: 0.038 - ETA: 4:54 - loss: 4.7690 - acc: 0.039 - ETA: 4:52 - loss: 4.7640 - acc: 0.039 - ETA: 4:49 - loss: 4.7618 - acc: 0.039 - ETA: 4:47 - loss: 4.7578 - acc: 0.040 - ETA: 4:44 - loss: 4.7509 - acc: 0.040 - ETA: 4:42 - loss: 4.7478 - acc: 0.040 - ETA: 4:39 - loss: 4.7466 - acc: 0.041 - ETA: 4:37 - loss: 4.7462 - acc: 0.040 - ETA: 4:34 - loss: 4.7423 - acc: 0.041 - ETA: 4:32 - loss: 4.7379 - acc: 0.041 - ETA: 4:29 - loss: 4.7346 - acc: 0.041 - ETA: 4:27 - loss: 4.7338 - acc: 0.041 - ETA: 4:24 - loss: 4.7311 - acc: 0.041 - ETA: 4:22 - loss: 4.7287 - acc: 0.041 - ETA: 4:19 - loss: 4.7257 - acc: 0.041 - ETA: 4:17 - loss: 4.7241 - acc: 0.041 - ETA: 4:14 - loss: 4.7210 - acc: 0.042 - ETA: 4:12 - loss: 4.7185 - acc: 0.043 - ETA: 4:09 - loss: 4.7156 - acc: 0.043 - ETA: 4:07 - loss: 4.7162 - acc: 0.043 - ETA: 4:05 - loss: 4.7153 - acc: 0.043 - ETA: 4:02 - loss: 4.7107 - acc: 0.043 - ETA: 4:00 - loss: 4.7099 - acc: 0.044 - ETA: 3:58 - loss: 4.7059 - acc: 0.044 - ETA: 3:55 - loss: 4.7009 - acc: 0.045 - ETA: 3:53 - loss: 4.6967 - acc: 0.045 - ETA: 3:50 - loss: 4.6959 - acc: 0.045 - ETA: 3:48 - loss: 4.6926 - acc: 0.045 - ETA: 3:45 - loss: 4.6905 - acc: 0.045 - ETA: 3:43 - loss: 4.6870 - acc: 0.045 - ETA: 3:40 - loss: 4.6862 - acc: 0.045 - ETA: 3:38 - loss: 4.6848 - acc: 0.045 - ETA: 3:35 - loss: 4.6813 - acc: 0.046 - ETA: 3:33 - loss: 4.6793 - acc: 0.046 - ETA: 3:30 - loss: 4.6765 - acc: 0.045 - ETA: 3:28 - loss: 4.6737 - acc: 0.046 - ETA: 3:25 - loss: 4.6699 - acc: 0.046 - ETA: 3:23 - loss: 4.6684 - acc: 0.046 - ETA: 3:20 - loss: 4.6663 - acc: 0.047 - ETA: 3:18 - loss: 4.6630 - acc: 0.047 - ETA: 3:16 - loss: 4.6616 - acc: 0.046 - ETA: 3:13 - loss: 4.6591 - acc: 0.047 - ETA: 3:11 - loss: 4.6571 - acc: 0.046 - ETA: 3:08 - loss: 4.6555 - acc: 0.046 - ETA: 3:06 - loss: 4.6539 - acc: 0.046 - ETA: 3:03 - loss: 4.6499 - acc: 0.047 - ETA: 3:01 - loss: 4.6467 - acc: 0.046 - ETA: 2:58 - loss: 4.6438 - acc: 0.046 - ETA: 2:56 - loss: 4.6388 - acc: 0.047 - ETA: 2:54 - loss: 4.6348 - acc: 0.049 - ETA: 2:51 - loss: 4.6320 - acc: 0.049 - ETA: 2:49 - loss: 4.6283 - acc: 0.050 - ETA: 2:46 - loss: 4.6260 - acc: 0.050 - ETA: 2:44 - loss: 4.6247 - acc: 0.050 - ETA: 2:41 - loss: 4.6218 - acc: 0.051 - ETA: 2:39 - loss: 4.6173 - acc: 0.051 - ETA: 2:36 - loss: 4.6141 - acc: 0.051 - ETA: 2:34 - loss: 4.6116 - acc: 0.052 - ETA: 2:31 - loss: 4.6097 - acc: 0.052 - ETA: 2:29 - loss: 4.6076 - acc: 0.052 - ETA: 2:27 - loss: 4.6047 - acc: 0.052 - ETA: 2:24 - loss: 4.6030 - acc: 0.053 - ETA: 2:22 - loss: 4.6005 - acc: 0.053 - ETA: 2:19 - loss: 4.5966 - acc: 0.053 - ETA: 2:17 - loss: 4.5947 - acc: 0.054 - ETA: 2:14 - loss: 4.5914 - acc: 0.053 - ETA: 2:12 - loss: 4.5901 - acc: 0.054 - ETA: 2:10 - loss: 4.5892 - acc: 0.054 - ETA: 2:07 - loss: 4.5862 - acc: 0.055 - ETA: 2:05 - loss: 4.5832 - acc: 0.056 - ETA: 2:02 - loss: 4.5795 - acc: 0.056 - ETA: 2:00 - loss: 4.5792 - acc: 0.056 - ETA: 1:57 - loss: 4.5771 - acc: 0.056 - ETA: 1:55 - loss: 4.5750 - acc: 0.057 - ETA: 1:53 - loss: 4.5735 - acc: 0.057 - ETA: 1:50 - loss: 4.5711 - acc: 0.057 - ETA: 1:48 - loss: 4.5687 - acc: 0.058 - ETA: 1:45 - loss: 4.5679 - acc: 0.057 - ETA: 1:43 - loss: 4.5663 - acc: 0.057 - ETA: 1:40 - loss: 4.5642 - acc: 0.058 - ETA: 1:38 - loss: 4.5632 - acc: 0.058 - ETA: 1:36 - loss: 4.5609 - acc: 0.058 - ETA: 1:33 - loss: 4.5584 - acc: 0.059 - ETA: 1:31 - loss: 4.5555 - acc: 0.059 - ETA: 1:28 - loss: 4.5544 - acc: 0.059 - ETA: 1:26 - loss: 4.5513 - acc: 0.059 - ETA: 1:23 - loss: 4.5486 - acc: 0.059 - ETA: 1:21 - loss: 4.5458 - acc: 0.060 - ETA: 1:19 - loss: 4.5439 - acc: 0.060 - ETA: 1:16 - loss: 4.5445 - acc: 0.060 - ETA: 1:14 - loss: 4.5428 - acc: 0.060 - ETA: 1:11 - loss: 4.5393 - acc: 0.061 - ETA: 1:09 - loss: 4.5359 - acc: 0.062 - ETA: 1:06 - loss: 4.5349 - acc: 0.061 - ETA: 1:04 - loss: 4.5332 - acc: 0.061 - ETA: 1:02 - loss: 4.5327 - acc: 0.061 - ETA: 59s - loss: 4.5308 - acc: 0.062 - ETA: 57s - loss: 4.5290 - acc: 0.06 - ETA: 54s - loss: 4.5278 - acc: 0.06 - ETA: 52s - loss: 4.5263 - acc: 0.06 - ETA: 49s - loss: 4.5238 - acc: 0.067155/7155 [==============================] - ETA: 47s - loss: 4.5202 - acc: 0.06 - ETA: 45s - loss: 4.5179 - acc: 0.06 - ETA: 42s - loss: 4.5157 - acc: 0.06 - ETA: 40s - loss: 4.5146 - acc: 0.06 - ETA: 37s - loss: 4.5135 - acc: 0.06 - ETA: 35s - loss: 4.5134 - acc: 0.06 - ETA: 32s - loss: 4.5134 - acc: 0.06 - ETA: 30s - loss: 4.5114 - acc: 0.06 - ETA: 28s - loss: 4.5102 - acc: 0.06 - ETA: 25s - loss: 4.5072 - acc: 0.06 - ETA: 23s - loss: 4.5058 - acc: 0.06 - ETA: 20s - loss: 4.5038 - acc: 0.06 - ETA: 18s - loss: 4.5020 - acc: 0.06 - ETA: 15s - loss: 4.5010 - acc: 0.06 - ETA: 13s - loss: 4.4988 - acc: 0.06 - ETA: 11s - loss: 4.4982 - acc: 0.06 - ETA: 8s - loss: 4.4948 - acc: 0.0669 - ETA: 6s - loss: 4.4951 - acc: 0.066 - ETA: 3s - loss: 4.4929 - acc: 0.067 - ETA: 1s - loss: 4.4909 - acc: 0.067 - 771s 108ms/step - loss: 4.4900 - acc: 0.0674 - val_loss: 4.1029 - val_acc: 0.1069\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6528/7155 [==========================>...] - ETA: 8:52 - loss: 3.4178 - acc: 0.187 - ETA: 8:52 - loss: 3.4675 - acc: 0.234 - ETA: 8:59 - loss: 3.3807 - acc: 0.270 - ETA: 8:58 - loss: 3.4387 - acc: 0.257 - ETA: 9:01 - loss: 3.4284 - acc: 0.256 - ETA: 9:02 - loss: 3.5142 - acc: 0.244 - ETA: 8:58 - loss: 3.5112 - acc: 0.241 - ETA: 8:54 - loss: 3.4970 - acc: 0.253 - ETA: 8:50 - loss: 3.4780 - acc: 0.263 - ETA: 8:47 - loss: 3.4492 - acc: 0.265 - ETA: 8:43 - loss: 3.4612 - acc: 0.264 - ETA: 8:40 - loss: 3.4677 - acc: 0.257 - ETA: 8:37 - loss: 3.4650 - acc: 0.257 - ETA: 8:34 - loss: 3.4699 - acc: 0.250 - ETA: 8:31 - loss: 3.4777 - acc: 0.245 - ETA: 8:28 - loss: 3.4658 - acc: 0.242 - ETA: 8:25 - loss: 3.4698 - acc: 0.237 - ETA: 8:27 - loss: 3.4747 - acc: 0.234 - ETA: 8:31 - loss: 3.4754 - acc: 0.230 - ETA: 8:28 - loss: 3.4774 - acc: 0.228 - ETA: 8:26 - loss: 3.4518 - acc: 0.238 - ETA: 8:32 - loss: 3.4628 - acc: 0.235 - ETA: 8:28 - loss: 3.4670 - acc: 0.237 - ETA: 8:24 - loss: 3.4573 - acc: 0.242 - ETA: 8:21 - loss: 3.4622 - acc: 0.240 - ETA: 8:18 - loss: 3.4513 - acc: 0.245 - ETA: 8:14 - loss: 3.4336 - acc: 0.251 - ETA: 8:11 - loss: 3.4301 - acc: 0.253 - ETA: 8:08 - loss: 3.4325 - acc: 0.253 - ETA: 8:05 - loss: 3.4411 - acc: 0.254 - ETA: 8:02 - loss: 3.4286 - acc: 0.255 - ETA: 7:58 - loss: 3.4308 - acc: 0.255 - ETA: 7:55 - loss: 3.4373 - acc: 0.253 - ETA: 7:52 - loss: 3.4352 - acc: 0.254 - ETA: 7:49 - loss: 3.4381 - acc: 0.252 - ETA: 7:46 - loss: 3.4462 - acc: 0.248 - ETA: 7:43 - loss: 3.4354 - acc: 0.249 - ETA: 7:40 - loss: 3.4336 - acc: 0.249 - ETA: 7:38 - loss: 3.4338 - acc: 0.248 - ETA: 7:35 - loss: 3.4331 - acc: 0.249 - ETA: 7:32 - loss: 3.4306 - acc: 0.249 - ETA: 7:29 - loss: 3.4256 - acc: 0.249 - ETA: 7:26 - loss: 3.4208 - acc: 0.250 - ETA: 7:23 - loss: 3.4267 - acc: 0.247 - ETA: 7:21 - loss: 3.4261 - acc: 0.247 - ETA: 7:18 - loss: 3.4214 - acc: 0.249 - ETA: 7:15 - loss: 3.4152 - acc: 0.250 - ETA: 7:12 - loss: 3.4132 - acc: 0.249 - ETA: 7:10 - loss: 3.4089 - acc: 0.249 - ETA: 7:07 - loss: 3.3982 - acc: 0.253 - ETA: 7:04 - loss: 3.3966 - acc: 0.254 - ETA: 7:02 - loss: 3.4006 - acc: 0.251 - ETA: 6:59 - loss: 3.4043 - acc: 0.251 - ETA: 6:57 - loss: 3.3984 - acc: 0.252 - ETA: 6:54 - loss: 3.3999 - acc: 0.248 - ETA: 6:51 - loss: 3.4019 - acc: 0.248 - ETA: 6:49 - loss: 3.4035 - acc: 0.249 - ETA: 6:46 - loss: 3.4032 - acc: 0.251 - ETA: 6:44 - loss: 3.4022 - acc: 0.252 - ETA: 6:41 - loss: 3.4043 - acc: 0.250 - ETA: 6:39 - loss: 3.4026 - acc: 0.251 - ETA: 6:36 - loss: 3.3999 - acc: 0.252 - ETA: 6:33 - loss: 3.4015 - acc: 0.253 - ETA: 6:31 - loss: 3.4045 - acc: 0.251 - ETA: 6:28 - loss: 3.4085 - acc: 0.251 - ETA: 6:26 - loss: 3.4119 - acc: 0.250 - ETA: 6:23 - loss: 3.4105 - acc: 0.250 - ETA: 6:21 - loss: 3.4099 - acc: 0.251 - ETA: 6:18 - loss: 3.4095 - acc: 0.251 - ETA: 6:16 - loss: 3.4074 - acc: 0.250 - ETA: 6:13 - loss: 3.4063 - acc: 0.252 - ETA: 6:10 - loss: 3.3998 - acc: 0.253 - ETA: 6:08 - loss: 3.4020 - acc: 0.253 - ETA: 6:05 - loss: 3.4048 - acc: 0.252 - ETA: 6:03 - loss: 3.4029 - acc: 0.252 - ETA: 6:00 - loss: 3.4028 - acc: 0.252 - ETA: 5:58 - loss: 3.4020 - acc: 0.252 - ETA: 5:56 - loss: 3.4007 - acc: 0.252 - ETA: 5:53 - loss: 3.3982 - acc: 0.252 - ETA: 5:51 - loss: 3.3965 - acc: 0.253 - ETA: 5:48 - loss: 3.3987 - acc: 0.253 - ETA: 5:46 - loss: 3.3982 - acc: 0.253 - ETA: 5:43 - loss: 3.4000 - acc: 0.252 - ETA: 5:41 - loss: 3.3993 - acc: 0.251 - ETA: 5:38 - loss: 3.3954 - acc: 0.253 - ETA: 5:36 - loss: 3.3986 - acc: 0.251 - ETA: 5:33 - loss: 3.3968 - acc: 0.252 - ETA: 5:31 - loss: 3.3931 - acc: 0.253 - ETA: 5:28 - loss: 3.3961 - acc: 0.253 - ETA: 5:26 - loss: 3.3936 - acc: 0.253 - ETA: 5:23 - loss: 3.3893 - acc: 0.253 - ETA: 5:21 - loss: 3.3856 - acc: 0.254 - ETA: 5:18 - loss: 3.3864 - acc: 0.253 - ETA: 5:16 - loss: 3.3869 - acc: 0.252 - ETA: 5:13 - loss: 3.3879 - acc: 0.252 - ETA: 5:11 - loss: 3.3853 - acc: 0.253 - ETA: 5:09 - loss: 3.3812 - acc: 0.254 - ETA: 5:06 - loss: 3.3800 - acc: 0.254 - ETA: 5:04 - loss: 3.3820 - acc: 0.253 - ETA: 5:01 - loss: 3.3834 - acc: 0.252 - ETA: 4:59 - loss: 3.3859 - acc: 0.251 - ETA: 4:56 - loss: 3.3836 - acc: 0.253 - ETA: 4:54 - loss: 3.3852 - acc: 0.252 - ETA: 4:51 - loss: 3.3853 - acc: 0.252 - ETA: 4:49 - loss: 3.3826 - acc: 0.253 - ETA: 4:46 - loss: 3.3803 - acc: 0.253 - ETA: 4:44 - loss: 3.3769 - acc: 0.254 - ETA: 4:41 - loss: 3.3745 - acc: 0.254 - ETA: 4:39 - loss: 3.3737 - acc: 0.255 - ETA: 4:37 - loss: 3.3705 - acc: 0.256 - ETA: 4:34 - loss: 3.3697 - acc: 0.255 - ETA: 4:32 - loss: 3.3739 - acc: 0.255 - ETA: 4:29 - loss: 3.3719 - acc: 0.256 - ETA: 4:27 - loss: 3.3721 - acc: 0.255 - ETA: 4:24 - loss: 3.3750 - acc: 0.255 - ETA: 4:22 - loss: 3.3762 - acc: 0.255 - ETA: 4:19 - loss: 3.3797 - acc: 0.255 - ETA: 4:17 - loss: 3.3824 - acc: 0.255 - ETA: 4:14 - loss: 3.3809 - acc: 0.255 - ETA: 4:12 - loss: 3.3807 - acc: 0.254 - ETA: 4:09 - loss: 3.3820 - acc: 0.254 - ETA: 4:07 - loss: 3.3806 - acc: 0.254 - ETA: 4:05 - loss: 3.3832 - acc: 0.253 - ETA: 4:02 - loss: 3.3827 - acc: 0.254 - ETA: 4:00 - loss: 3.3826 - acc: 0.254 - ETA: 3:57 - loss: 3.3784 - acc: 0.256 - ETA: 3:55 - loss: 3.3748 - acc: 0.257 - ETA: 3:52 - loss: 3.3740 - acc: 0.258 - ETA: 3:50 - loss: 3.3740 - acc: 0.257 - ETA: 3:47 - loss: 3.3744 - acc: 0.257 - ETA: 3:45 - loss: 3.3753 - acc: 0.256 - ETA: 3:43 - loss: 3.3772 - acc: 0.256 - ETA: 3:40 - loss: 3.3758 - acc: 0.256 - ETA: 3:38 - loss: 3.3749 - acc: 0.256 - ETA: 3:35 - loss: 3.3729 - acc: 0.256 - ETA: 3:33 - loss: 3.3722 - acc: 0.256 - ETA: 3:30 - loss: 3.3740 - acc: 0.255 - ETA: 3:28 - loss: 3.3715 - acc: 0.256 - ETA: 3:25 - loss: 3.3734 - acc: 0.255 - ETA: 3:23 - loss: 3.3755 - acc: 0.254 - ETA: 3:21 - loss: 3.3769 - acc: 0.254 - ETA: 3:18 - loss: 3.3772 - acc: 0.253 - ETA: 3:16 - loss: 3.3751 - acc: 0.253 - ETA: 3:13 - loss: 3.3772 - acc: 0.252 - ETA: 3:11 - loss: 3.3763 - acc: 0.253 - ETA: 3:09 - loss: 3.3762 - acc: 0.253 - ETA: 3:06 - loss: 3.3781 - acc: 0.253 - ETA: 3:04 - loss: 3.3755 - acc: 0.253 - ETA: 3:01 - loss: 3.3744 - acc: 0.253 - ETA: 2:59 - loss: 3.3740 - acc: 0.253 - ETA: 2:56 - loss: 3.3772 - acc: 0.252 - ETA: 2:54 - loss: 3.3750 - acc: 0.253 - ETA: 2:51 - loss: 3.3780 - acc: 0.252 - ETA: 2:49 - loss: 3.3787 - acc: 0.252 - ETA: 2:46 - loss: 3.3782 - acc: 0.252 - ETA: 2:44 - loss: 3.3777 - acc: 0.252 - ETA: 2:42 - loss: 3.3792 - acc: 0.251 - ETA: 2:39 - loss: 3.3795 - acc: 0.251 - ETA: 2:37 - loss: 3.3795 - acc: 0.251 - ETA: 2:34 - loss: 3.3784 - acc: 0.251 - ETA: 2:32 - loss: 3.3771 - acc: 0.250 - ETA: 2:29 - loss: 3.3770 - acc: 0.250 - ETA: 2:27 - loss: 3.3770 - acc: 0.251 - ETA: 2:24 - loss: 3.3750 - acc: 0.251 - ETA: 2:22 - loss: 3.3758 - acc: 0.250 - ETA: 2:20 - loss: 3.3785 - acc: 0.249 - ETA: 2:17 - loss: 3.3772 - acc: 0.249 - ETA: 2:15 - loss: 3.3756 - acc: 0.250 - ETA: 2:12 - loss: 3.3770 - acc: 0.249 - ETA: 2:10 - loss: 3.3782 - acc: 0.249 - ETA: 2:07 - loss: 3.3786 - acc: 0.249 - ETA: 2:05 - loss: 3.3803 - acc: 0.248 - ETA: 2:03 - loss: 3.3811 - acc: 0.248 - ETA: 2:00 - loss: 3.3806 - acc: 0.248 - ETA: 1:58 - loss: 3.3783 - acc: 0.248 - ETA: 1:55 - loss: 3.3802 - acc: 0.247 - ETA: 1:53 - loss: 3.3790 - acc: 0.248 - ETA: 1:50 - loss: 3.3791 - acc: 0.248 - ETA: 1:48 - loss: 3.3791 - acc: 0.247 - ETA: 1:45 - loss: 3.3764 - acc: 0.249 - ETA: 1:43 - loss: 3.3774 - acc: 0.248 - ETA: 1:41 - loss: 3.3770 - acc: 0.248 - ETA: 1:38 - loss: 3.3768 - acc: 0.248 - ETA: 1:36 - loss: 3.3744 - acc: 0.248 - ETA: 1:33 - loss: 3.3769 - acc: 0.247 - ETA: 1:31 - loss: 3.3766 - acc: 0.248 - ETA: 1:28 - loss: 3.3774 - acc: 0.247 - ETA: 1:26 - loss: 3.3763 - acc: 0.247 - ETA: 1:24 - loss: 3.3774 - acc: 0.247 - ETA: 1:21 - loss: 3.3765 - acc: 0.247 - ETA: 1:19 - loss: 3.3759 - acc: 0.247 - ETA: 1:16 - loss: 3.3767 - acc: 0.246 - ETA: 1:14 - loss: 3.3768 - acc: 0.246 - ETA: 1:11 - loss: 3.3759 - acc: 0.246 - ETA: 1:09 - loss: 3.3744 - acc: 0.246 - ETA: 1:07 - loss: 3.3743 - acc: 0.246 - ETA: 1:04 - loss: 3.3723 - acc: 0.246 - ETA: 1:02 - loss: 3.3729 - acc: 0.246 - ETA: 59s - loss: 3.3699 - acc: 0.247 - ETA: 57s - loss: 3.3689 - acc: 0.24 - ETA: 54s - loss: 3.3689 - acc: 0.24 - ETA: 52s - loss: 3.3708 - acc: 0.24 - ETA: 49s - loss: 3.3705 - acc: 0.24 - ETA: 47s - loss: 3.3705 - acc: 0.2457"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7155/7155 [==============================] - ETA: 45s - loss: 3.3711 - acc: 0.24 - ETA: 42s - loss: 3.3709 - acc: 0.24 - ETA: 40s - loss: 3.3694 - acc: 0.24 - ETA: 37s - loss: 3.3701 - acc: 0.24 - ETA: 35s - loss: 3.3693 - acc: 0.24 - ETA: 32s - loss: 3.3693 - acc: 0.24 - ETA: 30s - loss: 3.3673 - acc: 0.24 - ETA: 28s - loss: 3.3673 - acc: 0.24 - ETA: 25s - loss: 3.3672 - acc: 0.24 - ETA: 23s - loss: 3.3668 - acc: 0.24 - ETA: 20s - loss: 3.3674 - acc: 0.24 - ETA: 18s - loss: 3.3679 - acc: 0.24 - ETA: 15s - loss: 3.3690 - acc: 0.24 - ETA: 13s - loss: 3.3684 - acc: 0.24 - ETA: 11s - loss: 3.3678 - acc: 0.24 - ETA: 8s - loss: 3.3668 - acc: 0.2436 - ETA: 6s - loss: 3.3661 - acc: 0.243 - ETA: 3s - loss: 3.3645 - acc: 0.243 - ETA: 1s - loss: 3.3647 - acc: 0.243 - 772s 108ms/step - loss: 3.3652 - acc: 0.2436 - val_loss: 3.9299 - val_acc: 0.1379\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2194789c898>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, epochs=2, validation_data=(X_valid, Y_valid), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10357/10357 [==============================] - ETA: 16:3 - ETA: 14:3 - ETA: 13:5 - ETA: 13:3 - ETA: 13:2 - ETA: 13:1 - ETA: 13:0 - ETA: 13:0 - ETA: 13:0 - ETA: 12:5 - ETA: 12:5 - ETA: 12:4 - ETA: 12:4 - ETA: 12:3 - ETA: 12:3 - ETA: 12:3 - ETA: 12:3 - ETA: 12:2 - ETA: 12:2 - ETA: 12:2 - ETA: 12:1 - ETA: 12:1 - ETA: 12:1 - ETA: 12:0 - ETA: 12:0 - ETA: 12:0 - ETA: 12:0 - ETA: 11:5 - ETA: 11:5 - ETA: 11:5 - ETA: 11:5 - ETA: 11:4 - ETA: 11:4 - ETA: 11:4 - ETA: 11:4 - ETA: 11:3 - ETA: 11:3 - ETA: 11:3 - ETA: 11:3 - ETA: 11:2 - ETA: 11:2 - ETA: 11:2 - ETA: 11:2 - ETA: 11:1 - ETA: 11:1 - ETA: 11:1 - ETA: 11:0 - ETA: 11:0 - ETA: 11:0 - ETA: 11:0 - ETA: 10:5 - ETA: 10:5 - ETA: 10:5 - ETA: 10:5 - ETA: 10:4 - ETA: 10:4 - ETA: 10:4 - ETA: 10:4 - ETA: 10:4 - ETA: 10:3 - ETA: 10:3 - ETA: 10:3 - ETA: 10:3 - ETA: 10:2 - ETA: 10:2 - ETA: 10:2 - ETA: 10:2 - ETA: 10:1 - ETA: 10:1 - ETA: 10:1 - ETA: 10:1 - ETA: 10:0 - ETA: 10:0 - ETA: 10:0 - ETA: 10:0 - ETA: 9:5 - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 8: - ETA: 8: - ETA: 8: - ETA: 8: - ETA: 8: - ETA: 8: - ETA: 8: - ETA: 8: - ETA: 8: - ETA: 8: - ETA: 8: - ETA: 8: - ETA: 8: - ETA: 8: - ETA: 8: - ETA: 8: - ETA: 8: - ETA: 8: - ETA: 8: - ETA: 8: - ETA: 8: - ETA: 8: - ETA: 8: - ETA: 8: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 59s - ETA: 57 - ETA: 54 - ETA: 52 - ETA: 49 - ETA: 47 - ETA: 44 - ETA: 42 - ETA: 40 - ETA: 37 - ETA: 35 - ETA: 32 - ETA: 30 - ETA: 28 - ETA: 25 - ETA: 23 - ETA: 20 - ETA: 18 - ETA: 16 - ETA: 13 - ETA: 11 - ETA: 8 - ETA:  - ETA:  - ETA:  - 780s 75ms/step\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(x_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>affenpinscher</th>\n",
       "      <th>afghan_hound</th>\n",
       "      <th>african_hunting_dog</th>\n",
       "      <th>airedale</th>\n",
       "      <th>american_staffordshire_terrier</th>\n",
       "      <th>appenzeller</th>\n",
       "      <th>australian_terrier</th>\n",
       "      <th>basenji</th>\n",
       "      <th>basset</th>\n",
       "      <th>...</th>\n",
       "      <th>toy_poodle</th>\n",
       "      <th>toy_terrier</th>\n",
       "      <th>vizsla</th>\n",
       "      <th>walker_hound</th>\n",
       "      <th>weimaraner</th>\n",
       "      <th>welsh_springer_spaniel</th>\n",
       "      <th>west_highland_white_terrier</th>\n",
       "      <th>whippet</th>\n",
       "      <th>wire-haired_fox_terrier</th>\n",
       "      <th>yorkshire_terrier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000621fb3cbb32d8935728e48679680e</td>\n",
       "      <td>1.088201e-04</td>\n",
       "      <td>0.112157</td>\n",
       "      <td>2.492713e-05</td>\n",
       "      <td>0.000439</td>\n",
       "      <td>0.005048</td>\n",
       "      <td>0.001525</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.000540</td>\n",
       "      <td>0.042559</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>5.529162e-03</td>\n",
       "      <td>0.013911</td>\n",
       "      <td>0.013977</td>\n",
       "      <td>0.145787</td>\n",
       "      <td>0.000338</td>\n",
       "      <td>2.054683e-03</td>\n",
       "      <td>0.002179</td>\n",
       "      <td>0.000265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00102ee9d8eb90812350685311fe5890</td>\n",
       "      <td>2.386292e-04</td>\n",
       "      <td>0.000675</td>\n",
       "      <td>5.904982e-05</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000807</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.000234</td>\n",
       "      <td>0.000325</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000232</td>\n",
       "      <td>0.003723</td>\n",
       "      <td>2.381381e-04</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>0.000629</td>\n",
       "      <td>0.000586</td>\n",
       "      <td>0.055937</td>\n",
       "      <td>1.553115e-04</td>\n",
       "      <td>0.007895</td>\n",
       "      <td>0.000285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0012a730dfa437f5f3613fb75efcd4ce</td>\n",
       "      <td>1.622809e-02</td>\n",
       "      <td>0.020490</td>\n",
       "      <td>7.442918e-03</td>\n",
       "      <td>0.004689</td>\n",
       "      <td>0.005030</td>\n",
       "      <td>0.009980</td>\n",
       "      <td>0.003431</td>\n",
       "      <td>0.003835</td>\n",
       "      <td>0.008660</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001382</td>\n",
       "      <td>0.002253</td>\n",
       "      <td>5.708122e-03</td>\n",
       "      <td>0.008067</td>\n",
       "      <td>0.028801</td>\n",
       "      <td>0.006475</td>\n",
       "      <td>0.014692</td>\n",
       "      <td>9.404751e-03</td>\n",
       "      <td>0.005669</td>\n",
       "      <td>0.001258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>001510bc8570bbeee98c8d80c8a95ec1</td>\n",
       "      <td>8.543045e-03</td>\n",
       "      <td>0.083812</td>\n",
       "      <td>2.298049e-02</td>\n",
       "      <td>0.007074</td>\n",
       "      <td>0.001990</td>\n",
       "      <td>0.015656</td>\n",
       "      <td>0.008332</td>\n",
       "      <td>0.012429</td>\n",
       "      <td>0.002747</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001067</td>\n",
       "      <td>0.003120</td>\n",
       "      <td>1.198975e-03</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.004521</td>\n",
       "      <td>0.002342</td>\n",
       "      <td>0.001413</td>\n",
       "      <td>9.411016e-03</td>\n",
       "      <td>0.002331</td>\n",
       "      <td>0.002478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>001a5f3114548acdefa3d4da05474c2e</td>\n",
       "      <td>3.482509e-02</td>\n",
       "      <td>0.048724</td>\n",
       "      <td>1.045887e-02</td>\n",
       "      <td>0.003978</td>\n",
       "      <td>0.002588</td>\n",
       "      <td>0.002494</td>\n",
       "      <td>0.004769</td>\n",
       "      <td>0.000735</td>\n",
       "      <td>0.004713</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002015</td>\n",
       "      <td>0.000422</td>\n",
       "      <td>2.327428e-03</td>\n",
       "      <td>0.001368</td>\n",
       "      <td>0.008418</td>\n",
       "      <td>0.001030</td>\n",
       "      <td>0.004012</td>\n",
       "      <td>2.331804e-03</td>\n",
       "      <td>0.005400</td>\n",
       "      <td>0.003898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>00225dcd3e4d2410dd53239f95c0352f</td>\n",
       "      <td>3.295404e-04</td>\n",
       "      <td>0.093452</td>\n",
       "      <td>5.639387e-04</td>\n",
       "      <td>0.004062</td>\n",
       "      <td>0.001923</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.002035</td>\n",
       "      <td>0.001668</td>\n",
       "      <td>0.005525</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024099</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>9.658925e-03</td>\n",
       "      <td>0.009216</td>\n",
       "      <td>0.003989</td>\n",
       "      <td>0.012543</td>\n",
       "      <td>0.002463</td>\n",
       "      <td>5.717051e-03</td>\n",
       "      <td>0.029260</td>\n",
       "      <td>0.000624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>002c2a3117c2193b4d26400ce431eebd</td>\n",
       "      <td>3.944378e-02</td>\n",
       "      <td>0.051603</td>\n",
       "      <td>1.457914e-02</td>\n",
       "      <td>0.001961</td>\n",
       "      <td>0.001901</td>\n",
       "      <td>0.012848</td>\n",
       "      <td>0.024937</td>\n",
       "      <td>0.004418</td>\n",
       "      <td>0.006086</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000676</td>\n",
       "      <td>0.023375</td>\n",
       "      <td>1.864060e-03</td>\n",
       "      <td>0.000422</td>\n",
       "      <td>0.003628</td>\n",
       "      <td>0.004661</td>\n",
       "      <td>0.006083</td>\n",
       "      <td>2.170790e-03</td>\n",
       "      <td>0.002356</td>\n",
       "      <td>0.006403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>002c58d413a521ae8d1a5daeb35fc803</td>\n",
       "      <td>9.464670e-07</td>\n",
       "      <td>0.000174</td>\n",
       "      <td>4.205222e-08</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000578</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>3.139132e-07</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000233</td>\n",
       "      <td>0.532095</td>\n",
       "      <td>7.513113e-07</td>\n",
       "      <td>0.030788</td>\n",
       "      <td>0.001784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>002f80396f1e3db687c5932d7978b196</td>\n",
       "      <td>1.155102e-03</td>\n",
       "      <td>0.007629</td>\n",
       "      <td>4.484331e-02</td>\n",
       "      <td>0.006758</td>\n",
       "      <td>0.008428</td>\n",
       "      <td>0.001664</td>\n",
       "      <td>0.004302</td>\n",
       "      <td>0.008296</td>\n",
       "      <td>0.001908</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001010</td>\n",
       "      <td>0.000720</td>\n",
       "      <td>2.890339e-03</td>\n",
       "      <td>0.000868</td>\n",
       "      <td>0.037786</td>\n",
       "      <td>0.004499</td>\n",
       "      <td>0.005181</td>\n",
       "      <td>7.163113e-03</td>\n",
       "      <td>0.000403</td>\n",
       "      <td>0.000532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0036c6bcec6031be9e62a257b1c3c442</td>\n",
       "      <td>1.411590e-03</td>\n",
       "      <td>0.014303</td>\n",
       "      <td>2.637628e-03</td>\n",
       "      <td>0.001575</td>\n",
       "      <td>0.001414</td>\n",
       "      <td>0.002967</td>\n",
       "      <td>0.002179</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>0.005855</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004959</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>2.546252e-02</td>\n",
       "      <td>0.005061</td>\n",
       "      <td>0.030527</td>\n",
       "      <td>0.007271</td>\n",
       "      <td>0.001048</td>\n",
       "      <td>3.186298e-04</td>\n",
       "      <td>0.001383</td>\n",
       "      <td>0.002792</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 121 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id  affenpinscher  afghan_hound  \\\n",
       "0  000621fb3cbb32d8935728e48679680e   1.088201e-04      0.112157   \n",
       "1  00102ee9d8eb90812350685311fe5890   2.386292e-04      0.000675   \n",
       "2  0012a730dfa437f5f3613fb75efcd4ce   1.622809e-02      0.020490   \n",
       "3  001510bc8570bbeee98c8d80c8a95ec1   8.543045e-03      0.083812   \n",
       "4  001a5f3114548acdefa3d4da05474c2e   3.482509e-02      0.048724   \n",
       "5  00225dcd3e4d2410dd53239f95c0352f   3.295404e-04      0.093452   \n",
       "6  002c2a3117c2193b4d26400ce431eebd   3.944378e-02      0.051603   \n",
       "7  002c58d413a521ae8d1a5daeb35fc803   9.464670e-07      0.000174   \n",
       "8  002f80396f1e3db687c5932d7978b196   1.155102e-03      0.007629   \n",
       "9  0036c6bcec6031be9e62a257b1c3c442   1.411590e-03      0.014303   \n",
       "\n",
       "   african_hunting_dog  airedale  american_staffordshire_terrier  appenzeller  \\\n",
       "0         2.492713e-05  0.000439                        0.005048     0.001525   \n",
       "1         5.904982e-05  0.000011                        0.000807     0.000144   \n",
       "2         7.442918e-03  0.004689                        0.005030     0.009980   \n",
       "3         2.298049e-02  0.007074                        0.001990     0.015656   \n",
       "4         1.045887e-02  0.003978                        0.002588     0.002494   \n",
       "5         5.639387e-04  0.004062                        0.001923     0.000192   \n",
       "6         1.457914e-02  0.001961                        0.001901     0.012848   \n",
       "7         4.205222e-08  0.000004                        0.000105     0.000009   \n",
       "8         4.484331e-02  0.006758                        0.008428     0.001664   \n",
       "9         2.637628e-03  0.001575                        0.001414     0.002967   \n",
       "\n",
       "   australian_terrier   basenji    basset        ...          toy_poodle  \\\n",
       "0            0.000132  0.000540  0.042559        ...            0.000227   \n",
       "1            0.000234  0.000325  0.000150        ...            0.000232   \n",
       "2            0.003431  0.003835  0.008660        ...            0.001382   \n",
       "3            0.008332  0.012429  0.002747        ...            0.001067   \n",
       "4            0.004769  0.000735  0.004713        ...            0.002015   \n",
       "5            0.002035  0.001668  0.005525        ...            0.024099   \n",
       "6            0.024937  0.004418  0.006086        ...            0.000676   \n",
       "7            0.000578  0.000096  0.000003        ...            0.000074   \n",
       "8            0.004302  0.008296  0.001908        ...            0.001010   \n",
       "9            0.002179  0.000197  0.005855        ...            0.004959   \n",
       "\n",
       "   toy_terrier        vizsla  walker_hound  weimaraner  \\\n",
       "0     0.000054  5.529162e-03      0.013911    0.013977   \n",
       "1     0.003723  2.381381e-04      0.000193    0.000629   \n",
       "2     0.002253  5.708122e-03      0.008067    0.028801   \n",
       "3     0.003120  1.198975e-03      0.000292    0.004521   \n",
       "4     0.000422  2.327428e-03      0.001368    0.008418   \n",
       "5     0.000197  9.658925e-03      0.009216    0.003989   \n",
       "6     0.023375  1.864060e-03      0.000422    0.003628   \n",
       "7     0.000032  3.139132e-07      0.000007    0.000002   \n",
       "8     0.000720  2.890339e-03      0.000868    0.037786   \n",
       "9     0.000119  2.546252e-02      0.005061    0.030527   \n",
       "\n",
       "   welsh_springer_spaniel  west_highland_white_terrier       whippet  \\\n",
       "0                0.145787                     0.000338  2.054683e-03   \n",
       "1                0.000586                     0.055937  1.553115e-04   \n",
       "2                0.006475                     0.014692  9.404751e-03   \n",
       "3                0.002342                     0.001413  9.411016e-03   \n",
       "4                0.001030                     0.004012  2.331804e-03   \n",
       "5                0.012543                     0.002463  5.717051e-03   \n",
       "6                0.004661                     0.006083  2.170790e-03   \n",
       "7                0.000233                     0.532095  7.513113e-07   \n",
       "8                0.004499                     0.005181  7.163113e-03   \n",
       "9                0.007271                     0.001048  3.186298e-04   \n",
       "\n",
       "   wire-haired_fox_terrier  yorkshire_terrier  \n",
       "0                 0.002179           0.000265  \n",
       "1                 0.007895           0.000285  \n",
       "2                 0.005669           0.001258  \n",
       "3                 0.002331           0.002478  \n",
       "4                 0.005400           0.003898  \n",
       "5                 0.029260           0.000624  \n",
       "6                 0.002356           0.006403  \n",
       "7                 0.030788           0.001784  \n",
       "8                 0.000403           0.000532  \n",
       "9                 0.001383           0.002792  \n",
       "\n",
       "[10 rows x 121 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub = pd.DataFrame(preds)\n",
    "# Set column names to those generated by the one-hot encoding earlier\n",
    "col_names = one_hot.columns.values\n",
    "sub.columns = col_names\n",
    "# Insert the column id from the sample_submission at the start of the data frame\n",
    "sub.insert(0, 'id', df_test['id'])\n",
    "sub.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "# https://www.kaggle.com/c/dog-breed-identification/data\n",
    "\n",
    "\n",
    "# Kernel-1 link : https://www.kaggle.com/gaborfodor/dog-breed-pretrained-keras-models-lb-0-3\n",
    "# Kernel-2 link :  https://www.kaggle.com/orangutan/keras-vgg19-starter\n",
    "\n",
    "\n",
    "# References\n",
    "# Docker-python : https://github.com/kaggle/docker-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Licenses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by/3.0/us/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by/3.0/us/88x31.png\" /></a><br>The text in the document by NISHANT GOHEL and KARAN BHAVSAR is licensed under <a rel=\"license\" href=\"http://creativecommons.org/licenses/by/3.0/us/\">Creative Commons Attribution 3.0 United States License</a>.<br><br>\n",
    "\n",
    "\n",
    "The code in the document by NISHANT GOHEL and KARAN BHAVSAR is licensed under the MIT License https://opensource.org/licenses/MIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
