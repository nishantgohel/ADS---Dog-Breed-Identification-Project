{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook working is same as Dog-Breed-Identification - Using Keras but instead of using pretrained models of Keras \n",
    "# with bottleneck features we are designing and training our own CNN using AWS GPU and finding out accuracy on our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the data from the following link\n",
    "\n",
    "Dog Images : https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/dogImages.zip\n",
    "\n",
    "\n",
    "Download the dog dataset. Unzip the folder and place it in the repo, at location <i>path/to/dog-project/dogImages</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up AWS GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of training our model on a local CPU (or GPU), we have used Amazon Web Services to launch an EC2 GPU instance. In order to this you need to have a AWS account. Once you have created account successfully you will need to raise a ticket to AWS support center requesting to have access of <i>p2.xlarge</i> instance.\n",
    " \n",
    "After raising the ticket AWS will revert you back in 24 hrs for your access to the GPU. Once you receive the mail saying that your request has been approved follow the links below to setup GPU instance on Amazon Web Service.\n",
    " \n",
    "- Running Jupyter notebooks on GPU on AWS a starter guide : https://blog.keras.io/running-jupyter-notebooks-on-gpu-on-aws-a-starter-guide.html\n",
    "- Upload data from local machine to EC2 Instance : https://blog.dephyned.com/2014/01/30/upload-folder-from-local-machine-to-ec2-instance/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline  \n",
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking the Hardware Availabilty on our EC2 Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 251117513321490530, name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 11286970368\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 3698123135871704480\n",
       " physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Dog Dataset\n",
    "\n",
    "In the code cell below, we import a dataset of dog images. We populate a few variables through the use of the **load_files** function from the scikit-learn library:\n",
    "\n",
    "-  **train_files**, **valid_files**, **test_files** - numpy arrays containing file paths to images\n",
    "\n",
    "-  **train_targets**, **valid_targets**, **test_targets** - numpy arrays containing onehot-encoded classification labels\n",
    "\n",
    "-  **dog_names** - list of string-valued dog breed names for translating labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 133 total dog categories.\n",
      "There are 8351 total dog images.\n",
      "\n",
      "There are 6680 training dog images.\n",
      "There are 835 validation dog images.\n",
      "There are 836 test dog images.\n"
     ]
    }
   ],
   "source": [
    "# define function to load train, test, and validation datasets\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    dog_files = np.array(data['filenames'])\n",
    "    dog_targets = np_utils.to_categorical(np.array(data['target']), 133)\n",
    "    return dog_files, dog_targets\n",
    "\n",
    "# load train, test, and validation datasets\n",
    "train_files, train_targets = load_dataset('dogImages/train')\n",
    "valid_files, valid_targets = load_dataset('dogImages/valid')\n",
    "test_files, test_targets = load_dataset('dogImages/test')\n",
    "\n",
    "# load list of dog names\n",
    "dog_names = [item[20:-1] for item in sorted(glob(\"dogImages/train/*/\"))]\n",
    "\n",
    "# print statistics about the dataset\n",
    "print('There are %d total dog categories.' % len(dog_names))\n",
    "print('There are %s total dog images.\\n' % len(np.hstack([train_files, valid_files, test_files])))\n",
    "print('There are %d training dog images.' % len(train_files))\n",
    "print('There are %d validation dog images.' % len(valid_files))\n",
    "print('There are %d test dog images.'% len(test_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "When using TensorFlow as backend, Keras CNNs require a 4D array (which we'll also refer to as a 4D tensor) as input, with shape <br><br>\n",
    "\n",
    "<center>(nb_samples, rows,columns,channels)</center>\n",
    "\n",
    "where **nb_samples** corresponds to the total number of images, and rows, columns & channels correspond to the number of rows, columns, and channels for each image, respectively.<br>\n",
    "\n",
    "The **path_to_tensor** function below takes a string-valued file path to a color image as input and returns a 4D tensor suitable for supplying to a Keras CNN. The function first loads the image and resizes it to a square image that is 224 X 224 pixels. Next, the image is converted to an array, which is then resized to a 4D tensor. In this case, since we are working with color images, each image has three channels. Likewise, since we are processing a single image, the returned tensor will always have shape<br>\n",
    "\n",
    "<center>(1, 224, 224, 3).</center>\n",
    "\n",
    "The **paths_to_tensor** function takes a numpy array of string-valued image paths as input and returns a 4D tensor with shape<br><br>\n",
    "\n",
    "<center>(nb_samples, 224, 224, 3).</center>\n",
    "\n",
    "Here, **nb_samples** can be a single image or an array of image paths.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image                  \n",
    "from tqdm import tqdm\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we rescale the images by dividing every pixel in every image by 255\n",
    "\n",
    "Why we need rescaling ?\n",
    "\n",
    "Our original images consist in RGB coefficients in the 0-255, but such values would be too high for our model to process (given a typical learning rate), so we target values between 0 and 1 instead by scaling with a 1/255 factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6680/6680 [00:50<00:00, 132.12it/s]\n",
      "100%|██████████| 835/835 [00:05<00:00, 147.26it/s]\n",
      "100%|██████████| 836/836 [00:05<00:00, 148.92it/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "train_tensors = paths_to_tensor(train_files).astype('float32')/255\n",
    "valid_tensors = paths_to_tensor(valid_files).astype('float32')/255\n",
    "test_tensors = paths_to_tensor(test_files).astype('float32')/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmenting the Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Augmentation for Deep Learning\n",
    "\n",
    "Deep networks need large amount of training data to achieve good performance. To build a powerful image classifier using very little training data, image augmentation is usually required to boost the performance of deep networks. Image augmentation artificially creates training images through different ways of processing or combination of multiple processing, such as random rotation, shifts, shear and flips, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# create and configure augmented image generator\n",
    "datagen = ImageDataGenerator(\n",
    "    width_shift_range=0.1,  # randomly shift images horizontally (10% of total width)\n",
    "    height_shift_range=0.1,  # randomly shift images vertically (10% of total height)\n",
    "    horizontal_flip=True) # randomly flip images horizontally\n",
    "\n",
    "# fit augmented image generator on data\n",
    "datagen.fit(train_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining The CNN Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_1 (Batch (None, 224, 224, 3)       12        \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 222, 222, 16)      448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 111, 111, 16)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 111, 111, 16)      64        \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 109, 109, 32)      4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 54, 54, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 54, 54, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 52, 52, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 26, 26, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 26, 26, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 24, 24, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 10, 10, 256)       295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 5, 5, 256)         1024      \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense1 (Dense)               (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 133)               34181     \n",
      "=================================================================\n",
      "Total params: 494,577\n",
      "Trainable params: 493,579\n",
      "Non-trainable params: 998\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "model = Sequential()\n",
    "model.add(BatchNormalization(input_shape=(224, 224, 3)))\n",
    "model.add(Conv2D(filters=16, kernel_size=3, kernel_initializer='he_normal', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=3, kernel_initializer='he_normal', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))5\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=3, kernel_initializer='he_normal', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=3, kernel_initializer='he_normal', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(filters=256, kernel_size=3, kernel_initializer='he_normal', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(GlobalAveragePooling2D())\n",
    "\n",
    "\n",
    "# Dense layer with 256 neurons \n",
    "model.add(Dense(256, kernel_initializer='he_normal',name='dense1'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "model.add(Dense(133, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About The CNN Model\n",
    "\n",
    "\n",
    "Convolutional neural networks (CNNs) consist of multiple layers of receptive fields. These are small neuron collections which process portions of the input image. The outputs of these collections are then tiled so that their input regions overlap, to obtain a better representation of the original image; this is repeated for every such layer. One major advantage of convolutional networks is the use of shared weight in convolutional layers, which means that the same filter (weights bank) is used for each pixel in the layer; this both reduces memory footprint and improves performance. Different layers in CNN include Convolution, Dense,Batch Normalization, Pooling and Fully Connected layers.\n",
    "\n",
    "<img style=\"border-width:0\" src=\"https://cdn-images-1.medium.com/max/2000/1*XbuW8WuRrAY5pC4t-9DZAQ.jpeg\" />\n",
    "\n",
    "\n",
    "**Convolution:**\n",
    "\n",
    "The primary purpose of Convolution in case of a ConvNet is to extract features from the input image. Filters act as Feature detectors. The value of the filter, is in fact, not manually provided but the machine chooses the suitable value by training and changing its weights.\n",
    "\n",
    "**BatchNormalization:**\n",
    "\n",
    "A batch normalization layer normalizes each input channel across a mini-batch. The layer first normalizes the activations of each channel by subtracting the mini-batch mean and dividing by the mini-batch standard deviation. Then, the layer shifts the input by a learnable offset β and scales it by a learnable scale factor γ. Using batch normalization layers between convolutional layers and nonlinearities, such as ReLU layers, speeds up training of convolutional neural networks and reduces the sensitivity to network initialization.\n",
    "\n",
    "**Dense:**\n",
    "\n",
    "Dense layer comprises of the traditional fully connected neural network which classifies the input to their respective classes\n",
    "\n",
    "\n",
    "**Pooling**\n",
    "\n",
    "Pooling layer reduces the dimensionality of each feature map but retains the most important information. Pooling layer can be of different types: Max, Average, Sum etc.\n",
    "\n",
    "\n",
    "**Fully-Connected:**\n",
    "\n",
    "Finally, after several convolutional and max pooling layers, the high-level reasoning in the neural network is done via fully connected layers. A fully connected layer takes all neurons in the previous layer (be it fully connected, pooling, or convolutional) and connects it to every single neuron it has."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiling the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6680, 224, 224, 3) (6680, 133)\n"
     ]
    }
   ],
   "source": [
    "print(train_tensors.shape, train_targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train the model in the code cell below and we are using model checkpointing to save the model that attains the best validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "334/334 [==============================] - 79s 237ms/step - loss: 4.8443 - acc: 0.0241 - val_loss: 4.6170 - val_acc: 0.0383\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 4.61699, saving model to saved_models/weights.Assignment-4.5\n",
      "Epoch 2/20\n",
      "334/334 [==============================] - 76s 227ms/step - loss: 4.4520 - acc: 0.0475 - val_loss: 4.3040 - val_acc: 0.0743\n",
      "\n",
      "Epoch 00002: val_loss improved from 4.61699 to 4.30396, saving model to saved_models/weights.Assignment-4.5\n",
      "Epoch 3/20\n",
      "334/334 [==============================] - 76s 227ms/step - loss: 4.1983 - acc: 0.0636 - val_loss: 3.9580 - val_acc: 0.0850\n",
      "\n",
      "Epoch 00003: val_loss improved from 4.30396 to 3.95796, saving model to saved_models/weights.Assignment-4.5\n",
      "Epoch 4/20\n",
      "334/334 [==============================] - 76s 228ms/step - loss: 3.9889 - acc: 0.0873 - val_loss: 3.8462 - val_acc: 0.1174\n",
      "\n",
      "Epoch 00004: val_loss improved from 3.95796 to 3.84616, saving model to saved_models/weights.Assignment-4.5\n",
      "Epoch 5/20\n",
      "334/334 [==============================] - 76s 227ms/step - loss: 3.7777 - acc: 0.1145 - val_loss: 3.8214 - val_acc: 0.1222\n",
      "\n",
      "Epoch 00005: val_loss improved from 3.84616 to 3.82137, saving model to saved_models/weights.Assignment-4.5\n",
      "Epoch 6/20\n",
      "334/334 [==============================] - 76s 229ms/step - loss: 3.5944 - acc: 0.1377 - val_loss: 3.5721 - val_acc: 0.1521\n",
      "\n",
      "Epoch 00006: val_loss improved from 3.82137 to 3.57205, saving model to saved_models/weights.Assignment-4.5\n",
      "Epoch 7/20\n",
      "334/334 [==============================] - 76s 226ms/step - loss: 3.4150 - acc: 0.1650 - val_loss: 3.3118 - val_acc: 0.1880\n",
      "\n",
      "Epoch 00007: val_loss improved from 3.57205 to 3.31182, saving model to saved_models/weights.Assignment-4.5\n",
      "Epoch 8/20\n",
      "334/334 [==============================] - 76s 227ms/step - loss: 3.1908 - acc: 0.2055 - val_loss: 3.2851 - val_acc: 0.2012\n",
      "\n",
      "Epoch 00008: val_loss improved from 3.31182 to 3.28506, saving model to saved_models/weights.Assignment-4.5\n",
      "Epoch 9/20\n",
      "334/334 [==============================] - 76s 227ms/step - loss: 3.0087 - acc: 0.2361 - val_loss: 3.0829 - val_acc: 0.2467\n",
      "\n",
      "Epoch 00009: val_loss improved from 3.28506 to 3.08292, saving model to saved_models/weights.Assignment-4.5\n",
      "Epoch 10/20\n",
      "334/334 [==============================] - 76s 228ms/step - loss: 2.8788 - acc: 0.2597 - val_loss: 2.9939 - val_acc: 0.2731\n",
      "\n",
      "Epoch 00010: val_loss improved from 3.08292 to 2.99393, saving model to saved_models/weights.Assignment-4.5\n",
      "Epoch 11/20\n",
      "334/334 [==============================] - 76s 227ms/step - loss: 2.7108 - acc: 0.2937 - val_loss: 2.7545 - val_acc: 0.3042\n",
      "\n",
      "Epoch 00011: val_loss improved from 2.99393 to 2.75452, saving model to saved_models/weights.Assignment-4.5\n",
      "Epoch 12/20\n",
      "334/334 [==============================] - 77s 229ms/step - loss: 2.5682 - acc: 0.3165 - val_loss: 2.7249 - val_acc: 0.2874\n",
      "\n",
      "Epoch 00012: val_loss improved from 2.75452 to 2.72486, saving model to saved_models/weights.Assignment-4.5\n",
      "Epoch 13/20\n",
      "334/334 [==============================] - 77s 229ms/step - loss: 2.4311 - acc: 0.3479 - val_loss: 2.3822 - val_acc: 0.3641\n",
      "\n",
      "Epoch 00013: val_loss improved from 2.72486 to 2.38222, saving model to saved_models/weights.Assignment-4.5\n",
      "Epoch 14/20\n",
      "334/334 [==============================] - 76s 227ms/step - loss: 2.3424 - acc: 0.3672 - val_loss: 2.4323 - val_acc: 0.3665\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/20\n",
      "334/334 [==============================] - 76s 227ms/step - loss: 2.2058 - acc: 0.3948 - val_loss: 2.3112 - val_acc: 0.3796\n",
      "\n",
      "Epoch 00015: val_loss improved from 2.38222 to 2.31119, saving model to saved_models/weights.Assignment-4.5\n",
      "Epoch 16/20\n",
      "334/334 [==============================] - 76s 227ms/step - loss: 2.0824 - acc: 0.4204 - val_loss: 2.1857 - val_acc: 0.3988\n",
      "\n",
      "Epoch 00016: val_loss improved from 2.31119 to 2.18566, saving model to saved_models/weights.Assignment-4.5\n",
      "Epoch 17/20\n",
      "334/334 [==============================] - 76s 228ms/step - loss: 1.9812 - acc: 0.4434 - val_loss: 2.1535 - val_acc: 0.4072\n",
      "\n",
      "Epoch 00017: val_loss improved from 2.18566 to 2.15351, saving model to saved_models/weights.Assignment-4.5\n",
      "Epoch 18/20\n",
      "334/334 [==============================] - 76s 227ms/step - loss: 1.8874 - acc: 0.4663 - val_loss: 2.0070 - val_acc: 0.4731\n",
      "\n",
      "Epoch 00018: val_loss improved from 2.15351 to 2.00696, saving model to saved_models/weights.Assignment-4.5\n",
      "Epoch 19/20\n",
      "334/334 [==============================] - 76s 227ms/step - loss: 1.7618 - acc: 0.4973 - val_loss: 2.0100 - val_acc: 0.4635\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/20\n",
      "334/334 [==============================] - 76s 228ms/step - loss: 1.7194 - acc: 0.4975 - val_loss: 1.9250 - val_acc: 0.4922\n",
      "\n",
      "Epoch 00020: val_loss improved from 2.00696 to 1.92500, saving model to saved_models/weights.Assignment-4.5\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint  \n",
    "\n",
    "epochs = 20\n",
    "batch_size = 20\n",
    "\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.Assignment-4.5', verbose=1, save_best_only=True)\n",
    "\n",
    "history = model.fit_generator(datagen.flow(train_tensors, train_targets, batch_size=batch_size),\n",
    "                    validation_data=(valid_tensors, valid_targets), \n",
    "                    steps_per_epoch=train_tensors.shape[0] // batch_size,\n",
    "                    epochs=epochs, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('saved_models/weights.Assignment-4.5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calulating the Accuracy of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 47.0096%\n"
     ]
    }
   ],
   "source": [
    "# get index of predicted dog breed for each image in test set\n",
    "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model again with same no of epochs and batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "334/334 [==============================] - 78s 233ms/step - loss: 1.6255 - acc: 0.5332 - val_loss: 1.8885 - val_acc: 0.4647\n",
      "\n",
      "Epoch 00001: val_loss improved from 1.92500 to 1.88854, saving model to saved_models/weights.Assignment-4.5\n",
      "Epoch 2/20\n",
      "334/334 [==============================] - 76s 227ms/step - loss: 1.5761 - acc: 0.5374 - val_loss: 1.9149 - val_acc: 0.4850\n",
      "\n",
      "Epoch 00002: val_loss did not improve\n",
      "Epoch 3/20\n",
      "334/334 [==============================] - 76s 227ms/step - loss: 1.5239 - acc: 0.5476 - val_loss: 1.8440 - val_acc: 0.4946\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.88854 to 1.84403, saving model to saved_models/weights.Assignment-4.5\n",
      "Epoch 4/20\n",
      "334/334 [==============================] - 77s 230ms/step - loss: 1.4544 - acc: 0.5728 - val_loss: 1.9025 - val_acc: 0.4886\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/20\n",
      "334/334 [==============================] - 76s 226ms/step - loss: 1.4191 - acc: 0.5771 - val_loss: 1.8013 - val_acc: 0.5162\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.84403 to 1.80134, saving model to saved_models/weights.Assignment-4.5\n",
      "Epoch 6/20\n",
      "334/334 [==============================] - 77s 230ms/step - loss: 1.3479 - acc: 0.5945 - val_loss: 1.8557 - val_acc: 0.5018\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/20\n",
      "334/334 [==============================] - 76s 227ms/step - loss: 1.3128 - acc: 0.6075 - val_loss: 1.8396 - val_acc: 0.5078\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/20\n",
      "334/334 [==============================] - 77s 231ms/step - loss: 1.2760 - acc: 0.6172 - val_loss: 1.7991 - val_acc: 0.5257\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.80134 to 1.79910, saving model to saved_models/weights.Assignment-4.5\n",
      "Epoch 9/20\n",
      "334/334 [==============================] - 76s 229ms/step - loss: 1.2094 - acc: 0.6349 - val_loss: 1.6950 - val_acc: 0.5449\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.79910 to 1.69496, saving model to saved_models/weights.Assignment-4.5\n",
      "Epoch 10/20\n",
      "334/334 [==============================] - 76s 229ms/step - loss: 1.1911 - acc: 0.6395 - val_loss: 1.8395 - val_acc: 0.4994\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/20\n",
      "334/334 [==============================] - 76s 228ms/step - loss: 1.1580 - acc: 0.6488 - val_loss: 1.6806 - val_acc: 0.5293\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.69496 to 1.68065, saving model to saved_models/weights.Assignment-4.5\n",
      "Epoch 12/20\n",
      "334/334 [==============================] - 76s 229ms/step - loss: 1.1290 - acc: 0.6557 - val_loss: 1.6589 - val_acc: 0.5509\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.68065 to 1.65895, saving model to saved_models/weights.Assignment-4.5\n",
      "Epoch 13/20\n",
      "334/334 [==============================] - 76s 228ms/step - loss: 1.0851 - acc: 0.6696 - val_loss: 1.6199 - val_acc: 0.5713\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.65895 to 1.61990, saving model to saved_models/weights.Assignment-4.5\n",
      "Epoch 14/20\n",
      "334/334 [==============================] - 76s 229ms/step - loss: 1.0220 - acc: 0.6891 - val_loss: 1.7053 - val_acc: 0.5317\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/20\n",
      "334/334 [==============================] - 77s 230ms/step - loss: 1.0464 - acc: 0.6814 - val_loss: 1.6525 - val_acc: 0.5437\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/20\n",
      "334/334 [==============================] - 76s 226ms/step - loss: 1.0224 - acc: 0.6901 - val_loss: 1.7304 - val_acc: 0.5425\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/20\n",
      "334/334 [==============================] - 77s 230ms/step - loss: 1.0116 - acc: 0.6889 - val_loss: 1.6388 - val_acc: 0.5437\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/20\n",
      "334/334 [==============================] - 76s 227ms/step - loss: 0.9666 - acc: 0.6994 - val_loss: 1.7146 - val_acc: 0.5365\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/20\n",
      "334/334 [==============================] - 76s 226ms/step - loss: 0.9392 - acc: 0.7085 - val_loss: 1.7263 - val_acc: 0.5509\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/20\n",
      "334/334 [==============================] - 77s 230ms/step - loss: 0.9131 - acc: 0.7145 - val_loss: 1.6482 - val_acc: 0.5617\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 20\n",
    "\n",
    "history = model.fit_generator(datagen.flow(train_tensors, train_targets, batch_size=batch_size),\n",
    "                    validation_data=(valid_tensors, valid_targets), \n",
    "                    steps_per_epoch=train_tensors.shape[0] // batch_size,\n",
    "                    epochs=epochs, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model again with different no of epochs and batch size to see if accuracy improves or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "133/133 [==============================] - 76s 570ms/step - loss: 0.6598 - acc: 0.7929 - val_loss: 1.4724 - val_acc: 0.6048\n",
      "\n",
      "Epoch 00001: val_loss improved from 1.61990 to 1.47244, saving model to saved_models/weights.Assignment-4.5\n",
      "Epoch 2/25\n",
      "133/133 [==============================] - 75s 562ms/step - loss: 0.5672 - acc: 0.8149 - val_loss: 1.4722 - val_acc: 0.6120\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.47244 to 1.47221, saving model to saved_models/weights.Assignment-4.5\n",
      "Epoch 3/25\n",
      "133/133 [==============================] - 74s 556ms/step - loss: 0.5374 - acc: 0.8267 - val_loss: 1.4805 - val_acc: 0.6120\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/25\n",
      "133/133 [==============================] - 74s 557ms/step - loss: 0.5287 - acc: 0.8303 - val_loss: 1.4410 - val_acc: 0.6168\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.47221 to 1.44104, saving model to saved_models/weights.Assignment-4.5\n",
      "Epoch 5/25\n",
      "133/133 [==============================] - 74s 553ms/step - loss: 0.5180 - acc: 0.8276 - val_loss: 1.5502 - val_acc: 0.6072\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/25\n",
      "133/133 [==============================] - 75s 563ms/step - loss: 0.5030 - acc: 0.8417 - val_loss: 1.4906 - val_acc: 0.6323\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/25\n",
      "133/133 [==============================] - 74s 554ms/step - loss: 0.5007 - acc: 0.8359 - val_loss: 1.6090 - val_acc: 0.6072\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/25\n",
      "133/133 [==============================] - 74s 560ms/step - loss: 0.4921 - acc: 0.8390 - val_loss: 1.5373 - val_acc: 0.6132\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/25\n",
      "133/133 [==============================] - 74s 557ms/step - loss: 0.4770 - acc: 0.8422 - val_loss: 1.5623 - val_acc: 0.6251\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/25\n",
      "133/133 [==============================] - 74s 556ms/step - loss: 0.4852 - acc: 0.8465 - val_loss: 1.5820 - val_acc: 0.5904\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/25\n",
      "133/133 [==============================] - 74s 556ms/step - loss: 0.4726 - acc: 0.8471 - val_loss: 1.5415 - val_acc: 0.6287\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/25\n",
      "133/133 [==============================] - 74s 559ms/step - loss: 0.4614 - acc: 0.8511 - val_loss: 1.5626 - val_acc: 0.6024\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/25\n",
      "133/133 [==============================] - 74s 560ms/step - loss: 0.4509 - acc: 0.8506 - val_loss: 1.5844 - val_acc: 0.6192\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/25\n",
      "133/133 [==============================] - 74s 559ms/step - loss: 0.4580 - acc: 0.8518 - val_loss: 1.6380 - val_acc: 0.5988\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/25\n",
      "133/133 [==============================] - 75s 561ms/step - loss: 0.4388 - acc: 0.8593 - val_loss: 1.5781 - val_acc: 0.6204\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/25\n",
      "133/133 [==============================] - 74s 556ms/step - loss: 0.4276 - acc: 0.8616 - val_loss: 1.6064 - val_acc: 0.5976\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/25\n",
      "133/133 [==============================] - 74s 559ms/step - loss: 0.4167 - acc: 0.8671 - val_loss: 1.6444 - val_acc: 0.5976\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/25\n",
      "133/133 [==============================] - 74s 559ms/step - loss: 0.4125 - acc: 0.8655 - val_loss: 1.6980 - val_acc: 0.6060\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/25\n",
      "133/133 [==============================] - 74s 555ms/step - loss: 0.4165 - acc: 0.8626 - val_loss: 1.7511 - val_acc: 0.5856\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/25\n",
      "133/133 [==============================] - 74s 555ms/step - loss: 0.4217 - acc: 0.8599 - val_loss: 1.6530 - val_acc: 0.6060\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      "Epoch 21/25\n",
      "133/133 [==============================] - 74s 557ms/step - loss: 0.4197 - acc: 0.8598 - val_loss: 1.6307 - val_acc: 0.6072\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      "Epoch 22/25\n",
      "133/133 [==============================] - 74s 557ms/step - loss: 0.4047 - acc: 0.8619 - val_loss: 1.7112 - val_acc: 0.5856\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      "Epoch 23/25\n",
      "133/133 [==============================] - 74s 557ms/step - loss: 0.4203 - acc: 0.8593 - val_loss: 1.6715 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00023: val_loss did not improve\n",
      "Epoch 24/25\n",
      "133/133 [==============================] - 74s 554ms/step - loss: 0.4077 - acc: 0.8649 - val_loss: 1.8896 - val_acc: 0.5629\n",
      "\n",
      "Epoch 00024: val_loss did not improve\n",
      "Epoch 25/25\n",
      "133/133 [==============================] - 74s 557ms/step - loss: 0.3924 - acc: 0.8686 - val_loss: 1.8263 - val_acc: 0.5856\n",
      "\n",
      "Epoch 00025: val_loss did not improve\n"
     ]
    }
   ],
   "source": [
    "epochs = 25\n",
    "batch_size = 50\n",
    "\n",
    "history = model.fit_generator(datagen.flow(train_tensors, train_targets, batch_size=batch_size),\n",
    "                    validation_data=(valid_tensors, valid_targets), \n",
    "                    steps_per_epoch=train_tensors.shape[0] // batch_size,\n",
    "                    epochs=epochs, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "133/133 [==============================] - 75s 561ms/step - loss: 0.3921 - acc: 0.8688 - val_loss: 1.6847 - val_acc: 0.5892\n",
      "\n",
      "Epoch 00001: val_loss did not improve\n",
      "Epoch 2/25\n",
      "133/133 [==============================] - 74s 560ms/step - loss: 0.3937 - acc: 0.8709 - val_loss: 1.7439 - val_acc: 0.5928\n",
      "\n",
      "Epoch 00002: val_loss did not improve\n",
      "Epoch 3/25\n",
      "133/133 [==============================] - 74s 554ms/step - loss: 0.3764 - acc: 0.8761 - val_loss: 1.8247 - val_acc: 0.5808\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/25\n",
      "133/133 [==============================] - 74s 558ms/step - loss: 0.3360 - acc: 0.8892 - val_loss: 1.6993 - val_acc: 0.5928\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/25\n",
      "133/133 [==============================] - 74s 558ms/step - loss: 0.3675 - acc: 0.8786 - val_loss: 1.7735 - val_acc: 0.6036\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/25\n",
      "133/133 [==============================] - 74s 556ms/step - loss: 0.3621 - acc: 0.8778 - val_loss: 1.7525 - val_acc: 0.6012\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/25\n",
      "133/133 [==============================] - 74s 557ms/step - loss: 0.3636 - acc: 0.8830 - val_loss: 1.8267 - val_acc: 0.6012\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/25\n",
      "133/133 [==============================] - 74s 559ms/step - loss: 0.3525 - acc: 0.8834 - val_loss: 1.7478 - val_acc: 0.5868\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/25\n",
      "133/133 [==============================] - 74s 555ms/step - loss: 0.3340 - acc: 0.8909 - val_loss: 1.7737 - val_acc: 0.5904\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/25\n",
      "133/133 [==============================] - 74s 559ms/step - loss: 0.3479 - acc: 0.8874 - val_loss: 1.8402 - val_acc: 0.6168\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/25\n",
      "133/133 [==============================] - 74s 556ms/step - loss: 0.3303 - acc: 0.8894 - val_loss: 1.8172 - val_acc: 0.5832\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/25\n",
      "133/133 [==============================] - 74s 557ms/step - loss: 0.3427 - acc: 0.8879 - val_loss: 1.7899 - val_acc: 0.5725\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/25\n",
      "133/133 [==============================] - 74s 554ms/step - loss: 0.3220 - acc: 0.8979 - val_loss: 1.8314 - val_acc: 0.5964\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/25\n",
      "133/133 [==============================] - 74s 559ms/step - loss: 0.3447 - acc: 0.8861 - val_loss: 1.7642 - val_acc: 0.6156\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/25\n",
      "133/133 [==============================] - 74s 557ms/step - loss: 0.3397 - acc: 0.8873 - val_loss: 1.8446 - val_acc: 0.5856\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/25\n",
      "133/133 [==============================] - 74s 555ms/step - loss: 0.3110 - acc: 0.8978 - val_loss: 1.8177 - val_acc: 0.5976\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/25\n",
      "133/133 [==============================] - 74s 558ms/step - loss: 0.2913 - acc: 0.9048 - val_loss: 1.9129 - val_acc: 0.5892\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/25\n",
      "133/133 [==============================] - 74s 556ms/step - loss: 0.3272 - acc: 0.8901 - val_loss: 1.8188 - val_acc: 0.5964\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/25\n",
      "133/133 [==============================] - 75s 563ms/step - loss: 0.3038 - acc: 0.8991 - val_loss: 1.8797 - val_acc: 0.5892\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/25\n",
      "133/133 [==============================] - 74s 558ms/step - loss: 0.3150 - acc: 0.8992 - val_loss: 1.7959 - val_acc: 0.6180\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      "Epoch 21/25\n",
      "133/133 [==============================] - 74s 554ms/step - loss: 0.3026 - acc: 0.9030 - val_loss: 1.7364 - val_acc: 0.6228\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      "Epoch 22/25\n",
      "133/133 [==============================] - 75s 561ms/step - loss: 0.3201 - acc: 0.8955 - val_loss: 1.8696 - val_acc: 0.5880\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      "Epoch 23/25\n",
      "133/133 [==============================] - 74s 556ms/step - loss: 0.3086 - acc: 0.8998 - val_loss: 1.8542 - val_acc: 0.6048\n",
      "\n",
      "Epoch 00023: val_loss did not improve\n",
      "Epoch 24/25\n",
      "133/133 [==============================] - 75s 562ms/step - loss: 0.2909 - acc: 0.9047 - val_loss: 1.8173 - val_acc: 0.5952\n",
      "\n",
      "Epoch 00024: val_loss did not improve\n",
      "Epoch 25/25\n",
      "133/133 [==============================] - 74s 557ms/step - loss: 0.3057 - acc: 0.8984 - val_loss: 2.0513 - val_acc: 0.5701\n",
      "\n",
      "Epoch 00025: val_loss did not improve\n"
     ]
    }
   ],
   "source": [
    "epochs = 25\n",
    "batch_size = 50\n",
    "\n",
    "history = model.fit_generator(datagen.flow(train_tensors, train_targets, batch_size=batch_size),\n",
    "                    validation_data=(valid_tensors, valid_targets), \n",
    "                    steps_per_epoch=train_tensors.shape[0] // batch_size,\n",
    "                    epochs=epochs, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calulating the Accuracy of Model Again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 60.8852%\n"
     ]
    }
   ],
   "source": [
    "# get index of predicted dog breed for each image in test set\n",
    "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the graph for Accuracy vs Validation Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the graph for Loss vs Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset \n",
    "\n",
    "# Dog Images :  https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/dogImages.zip\n",
    "\n",
    "\n",
    "\n",
    "# References \n",
    "\n",
    "# https://www.researchgate.net/publication/283813525_Dog_breed_classification_via_landmarks\n",
    "# https://web.stanford.edu/class/cs231a/prev_projects_2016/output%20(1).pdf\n",
    "# http://cs231n.stanford.edu/reports/2015/pdfs/automatic-dog-breed.pdf\n",
    "# https://pdfs.semanticscholar.org/d58a/663d775c3ef6398d605b6d6dbd02fb3d8725.pdf\n",
    "# https://github.com/nikbearbrown/NEU_COE\n",
    "# Keras Documentation for Designing our own CNN : https://keras.io/\n",
    "# Batch normalization in Neural Networks : https://towardsdatascience.com/batch-normalization-in-neural-networks-1ac91516821c\n",
    "# Image Augmenatation : https://towardsdatascience.com/image-augmentation-for-deep-learning-histogram-equalization-a71387f609b2\n",
    "# Image Augmentation for Deep Learning With Keras : https://machinelearningmastery.com/image-augmentation-deep-learning-keras/\n",
    "# Deep learning and the rise of the GPU : https://www.ibm.com/developerworks/library/cc-machine-learning-deep-learning-architectures/index.html\n",
    "# CNN : https://en.wikipedia.org/wiki/Convolutional_neural_network\n",
    "# Deep learning architectures : https://www.ibm.com/developerworks/library/cc-machine-learning-deep-learning-architectures/index.html\n",
    "\n",
    "# YouTube References\n",
    "\n",
    "# https://www.youtube.com/watch?v=aircAruvnKk\n",
    "# https://www.youtube.com/watch?v=IHZwWFHWa-w\n",
    "# https://www.youtube.com/watch?v=Ilg3gGewQ5U\n",
    "# MIT 6.S191: Introduction to Deep Learning https://youtu.be/JN6H4rQvwgY\n",
    "# TensorFlow 101 (Really Awesome Intro Into TensorFlow) https://youtu.be/oxf3o8IbCk4\n",
    "# Introduction to TensorFlow Tutorials https://youtu.be/er8RQZoX3yk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Licenses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by/3.0/us/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by/3.0/us/88x31.png\" /></a><br>The text in the document by NISHANT GOHEL and KARAN BHAVSAR is licensed under <a rel=\"license\" href=\"http://creativecommons.org/licenses/by/3.0/us/\">Creative Commons Attribution 3.0 United States License</a>.<br><br>\n",
    "\n",
    "\n",
    "The code in the document by NISHANT GOHEL and KARAN BHAVSAR is licensed under the MIT License https://opensource.org/licenses/MIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
